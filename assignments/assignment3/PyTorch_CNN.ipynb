{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_AEuBJucTyB",
        "colab_type": "text"
      },
      "source": [
        "# Задание 3.2 - сверточные нейронные сети (CNNs) в PyTorch\n",
        "\n",
        "Это упражнение мы буде выполнять в Google Colab - https://colab.research.google.com/  \n",
        "Google Colab позволяет запускать код в notebook в облаке Google, где можно воспользоваться бесплатным GPU!  \n",
        "\n",
        "Авторы курса благодарят компанию Google и надеятся, что праздник не закончится.\n",
        "\n",
        "Туториал по настройке Google Colab:  \n",
        "https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d  \n",
        "(Keras инсталлировать не нужно, наш notebook сам установит PyTorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FcXBeP1O7cnY",
        "outputId": "87a5d7b9-5c74-44a6-e6d2-8aafcd3cd04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "# Intstall PyTorch and download data\n",
        "!pip3 install torch torchvision\n",
        "\n",
        "!wget -c http://ufldl.stanford.edu/housenumbers/train_32x32.mat http://ufldl.stanford.edu/housenumbers/test_32x32.mat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "--2020-03-27 19:21:55--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat     100%[===================>] 173.61M  14.4MB/s    in 11s     \n",
            "\n",
            "2020-03-27 19:22:07 (15.6 MB/s) - ‘train_32x32.mat’ saved [182040794/182040794]\n",
            "\n",
            "--2020-03-27 19:22:07--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Reusing existing connection to ufldl.stanford.edu:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64275384 (61M) [text/plain]\n",
            "Saving to: ‘test_32x32.mat’\n",
            "\n",
            "test_32x32.mat      100%[===================>]  61.30M  18.5MB/s    in 3.3s    \n",
            "\n",
            "2020-03-27 19:22:10 (18.4 MB/s) - ‘test_32x32.mat’ saved [64275384/64275384]\n",
            "\n",
            "FINISHED --2020-03-27 19:22:10--\n",
            "Total wall clock time: 15s\n",
            "Downloaded: 2 files, 235M in 14s (16.2 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-afwWw-Q85vD",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from torchvision import transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NNU-OD9O9ltP",
        "outputId": "9713650b-7d28-4591-8e23-c588f813c090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\") # Let's make sure GPU is available!\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXoZddKcTyR",
        "colab_type": "text"
      },
      "source": [
        "# Загружаем данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YAvkoRx-9FsP",
        "colab": {}
      },
      "source": [
        "\n",
        "# First, lets load the dataset\n",
        "data_train = dset.SVHN('./', \n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
        "                                               std=[0.20,0.20,0.20])                           \n",
        "                       ])\n",
        "                      )\n",
        "data_test = dset.SVHN('./', split='test', transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
        "                                               std=[0.20,0.20,0.20])                           \n",
        "                       ]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s1blPg1cTyW",
        "colab_type": "text"
      },
      "source": [
        "Разделяем данные на training и validation.\n",
        "\n",
        "На всякий случай для подробностей - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YRnr8CPg7Hli",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "data_size = data_train.data.shape[0]\n",
        "validation_split = .2\n",
        "split = int(np.floor(validation_split * data_size))\n",
        "indices = list(range(data_size))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
        "                                         sampler=val_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyYvt-T67PBG",
        "colab": {}
      },
      "source": [
        "# We'll use a special helper module to shape it into a flat tensor\n",
        "class Flattener(nn.Module):\n",
        "    def forward(self, x):\n",
        "        batch_size, *_ = x.shape\n",
        "        return x.view(batch_size, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVKcnYtacTyh",
        "colab_type": "text"
      },
      "source": [
        "Создадим простейшую сеть с новыми слоями:  \n",
        "Convolutional - `nn.Conv2d`  \n",
        "MaxPool - `nn.MaxPool2d`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9SFVGZP7SQd",
        "colab": {}
      },
      "source": [
        "nn_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(4),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(4),    \n",
        "            Flattener(),\n",
        "            nn.Linear(64*2*2, 10),\n",
        "          )\n",
        "\n",
        "nn_model.type(torch.cuda.FloatTensor)\n",
        "nn_model.to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=1e-1, weight_decay=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1IN2_qOcTym",
        "colab_type": "text"
      },
      "source": [
        "Восстановите функцию `compute_accuracy` из прошлого задания.  \n",
        "Единственное отличие в новом - она должна передать данные на GPU прежде чем прогонять через модель. Сделайте это так же, как это делает функция `train_model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ek3KVQK7hJ6",
        "colab": {}
      },
      "source": [
        "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler=None):    \n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Enter train mode\n",
        "        \n",
        "        loss_accum = 0\n",
        "        correct_samples = 0\n",
        "        total_samples = 0\n",
        "        for i_step, (x, y) in enumerate(train_loader):\n",
        "          \n",
        "            x_gpu = x.to(device)\n",
        "            y_gpu = y.to(device)\n",
        "            prediction = model(x_gpu)    \n",
        "            loss_value = loss(prediction, y_gpu)\n",
        "            optimizer.zero_grad()\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            _, indices = torch.max(prediction, 1)\n",
        "            correct_samples += torch.sum(indices == y_gpu)\n",
        "            total_samples += y.shape[0]\n",
        "            \n",
        "            loss_accum += loss_value\n",
        "\n",
        "        ave_loss = loss_accum / i_step\n",
        "        train_accuracy = float(correct_samples) / total_samples\n",
        "        val_accuracy = compute_accuracy(model, val_loader)\n",
        "        \n",
        "        loss_history.append(float(ave_loss))\n",
        "        train_history.append(train_accuracy)\n",
        "        val_history.append(val_accuracy)\n",
        "        \n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
        "        \n",
        "    return loss_history, train_history, val_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaySMpIdcTyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(model, loader):\n",
        "    \"\"\"\n",
        "    Computes accuracy on the dataset wrapped in a loader\n",
        "    \n",
        "    Returns: accuracy as a float value between 0 and 1\n",
        "    \"\"\"\n",
        "    model.eval() # Evaluation mode\n",
        "    # Don't forget to move the data to device before running it through the model!    \n",
        "    val_accuracy, bingo, total = 0, 0, 0\n",
        "        \n",
        "    for x, y in loader:\n",
        "        x_on_xpu, y_on_gpu = x.to(device), y.to(device)\n",
        "        prediction = model(x_on_xpu)\n",
        "        _, indices = torch.max(prediction, 1)\n",
        "        bingo += torch.sum(indices == y_on_gpu)\n",
        "        total += y_on_gpu.shape[0]\n",
        "        val_accuracy = float(bingo)/total\n",
        "    return val_accuracy\n",
        "        \n",
        "def multiclass_accuracy(pred, ground_truth):\n",
        "    return np.mean([p == gt for (p, gt) in zip(pred, ground_truth)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OiVCCqctcTyv",
        "outputId": "0e89fd45-4b88-4fac-d9a8-8db91cef7739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 1.397440, Train accuracy: 0.534689, Val accuracy: 0.757013\n",
            "Average loss: 0.702144, Train accuracy: 0.787018, Val accuracy: 0.794553\n",
            "Average loss: 0.588726, Train accuracy: 0.821554, Val accuracy: 0.830660\n",
            "Average loss: 0.537585, Train accuracy: 0.838600, Val accuracy: 0.833322\n",
            "Average loss: 0.504214, Train accuracy: 0.850015, Val accuracy: 0.846290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6a-3a1ZFGEw_"
      },
      "source": [
        "# Аугментация данных (Data augmentation)\n",
        "\n",
        "В работе с изображениями одним из особенно важных методов является аугментация данных - то есть, генерация дополнительных данных для тренировки на основе изначальных.   \n",
        "Таким образом, мы получаем возможность \"увеличить\" набор данных для тренировки, что ведет к лучшей работе сети.\n",
        "Важно, чтобы аугментированные данные были похожи на те, которые могут встретиться в реальной жизни, иначе польза от аугментаций уменьшается и может ухудшить работу сети.\n",
        "\n",
        "С PyTorch идут несколько таких алгоритмов, называемых `transforms`. Более подробно про них можно прочитать тут -\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
        "\n",
        "Ниже мы используем следующие алгоритмы генерации:\n",
        "- ColorJitter - случайное изменение цвета\n",
        "- RandomHorizontalFlip - горизонтальное отражение с вероятностью 50%\n",
        "- RandomVerticalFlip - вертикальное отражение с вероятностью 50%\n",
        "- RandomRotation - случайный поворот"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCWMUWmr7t5g",
        "colab": {}
      },
      "source": [
        "tfs = transforms.Compose([\n",
        "    transforms.ColorJitter(hue=.50, saturation=.50),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(50, resample=PIL.Image.BILINEAR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43,0.44,0.47],\n",
        "                       std=[0.20,0.20,0.20])                           \n",
        "])\n",
        "\n",
        "# Create augmented train dataset\n",
        "data_aug_train = dset.SVHN('./', \n",
        "                       transform=tfs\n",
        "                      )\n",
        "\n",
        "train_aug_loader = torch.utils.data.DataLoader(data_aug_train, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3joUMaIjcTy4",
        "colab_type": "text"
      },
      "source": [
        "Визуализируем результаты агментации (вообще, смотреть на сгенерированные данные всегда очень полезно)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YlJJEro1KZ45",
        "outputId": "caa0c562-b577-46dd-e57f-9fd634b2cd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# TODO: Visualize some augmented images!\n",
        "# hint: you can create new datasets and loaders to accomplish this\n",
        "\n",
        "# Based on the visualizations, should we keep all the augmentations?\n",
        "\n",
        "tfs = transforms.Compose([\n",
        "    transforms.ColorJitter(hue=.20, saturation=.20),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
        "])\n",
        "\n",
        "data_aug_vis = dset.SVHN('./', \n",
        "                       transform=tfs\n",
        "                      )\n",
        "\n",
        "plt.figure(figsize=(30, 3))\n",
        "\n",
        "for i, (x, y) in enumerate(data_aug_vis):\n",
        "    if i == 10:\n",
        "        break\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x)\n",
        "    plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABpgAAACcCAYAAABr5qh0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny0wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 2160x216 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o2LrmsYHoguB"
      },
      "source": [
        "Все ли агментации одинаково полезны на этом наборе данных? Могут ли быть среди них те, которые собьют модель с толку?\n",
        "\n",
        "Выберите из них только корректные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "evro9ksXGs9u",
        "colab": {}
      },
      "source": [
        "# TODO: \n",
        "tfs = transforms.Compose([\n",
        "    # TODO: Add good augmentations\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43,0.44,0.47],\n",
        "                       std=[0.20,0.20,0.20])                           \n",
        "])\n",
        "\n",
        "# TODO create new instances of loaders with the augmentations you chose\n",
        "data_aug_vis = dset.SVHN('./', transform=tfs)\n",
        "train_aug_loader = torch.utils.data.DataLoader(data_aug_vis,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler=train_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PeO6Zw0DHqPR",
        "outputId": "b985f843-02b5-4f1c-9cb2-801f358e21ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Finally, let's train with augmentations!\n",
        "\n",
        "# Note we shouldn't use augmentations on validation\n",
        "\n",
        "loss_history, train_history, val_history = train_model(nn_model, train_aug_loader, val_loader, loss, optimizer, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 0.483274, Train accuracy: 0.856551, Val accuracy: 0.842195\n",
            "Average loss: 0.462786, Train accuracy: 0.861584, Val accuracy: 0.836257\n",
            "Average loss: 0.445532, Train accuracy: 0.867010, Val accuracy: 0.844311\n",
            "Average loss: 0.429155, Train accuracy: 0.871703, Val accuracy: 0.848884\n",
            "Average loss: 0.422868, Train accuracy: 0.874450, Val accuracy: 0.834755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r0bcioK6JBDK"
      },
      "source": [
        "# LeNet\n",
        "Попробуем имплементировать классическую архитектуру сверточной нейронной сети, предложенную Яном ЛеКуном в 1998 году. В свое время она достигла впечатляющих результатов на MNIST, посмотрим как она справится с SVHN?\n",
        "Она описана в статье [\"Gradient Based Learning Applied to Document Recognition\"](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), попробуйте прочитать ключевые части и имплементировать предложенную архитетуру на PyTorch.\n",
        "\n",
        "Реализовывать слои и функцию ошибки LeNet, которых нет в PyTorch, **не нужно** - просто возьмите их размеры и переведите в уже известные нам Convolutional, Pooling и Fully Connected layers.\n",
        "\n",
        "Если в статье не очень понятно, можно просто погуглить LeNet и разобраться в деталях :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ieEzZUglJAUB",
        "colab": {}
      },
      "source": [
        "# TODO: Implement LeNet-like architecture for SVHN task\n",
        "lenet_model = nn.Sequential(\n",
        "    nn.Conv2d(3, 6, 5),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.MaxPool2d(2, stride=2),\n",
        "    nn.Conv2d(6, 16, 5),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.MaxPool2d(2, stride=2),\n",
        "    nn.Conv2d(16, 120, 5),\n",
        "    nn.ReLU(inplace=True),\n",
        "    Flattener(),\n",
        "    nn.Linear(120, 84),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(84, 10))\n",
        "\n",
        "lenet_model.type(torch.cuda.FloatTensor)\n",
        "lenet_model.to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
        "optimizer = optim.SGD(lenet_model.parameters(), lr=1e-1, weight_decay=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WMmaPfdeKk9H",
        "outputId": "03995ebc-6212-439f-94df-28c5dfa13d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Let's train it!\n",
        "loss_history, train_history, val_history = train_model(\n",
        "                                                    lenet_model,\n",
        "                                                    train_aug_loader,\n",
        "                                                    val_loader,\n",
        "                                                    loss,\n",
        "                                                    optimizer,\n",
        "                                                    10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 1.203883, Train accuracy: 0.600092, Val accuracy: 0.836530\n",
            "Average loss: 0.487228, Train accuracy: 0.855458, Val accuracy: 0.860624\n",
            "Average loss: 0.399517, Train accuracy: 0.881138, Val accuracy: 0.877892\n",
            "Average loss: 0.344504, Train accuracy: 0.896683, Val accuracy: 0.872091\n",
            "Average loss: 0.310789, Train accuracy: 0.905812, Val accuracy: 0.878302\n",
            "Average loss: 0.281920, Train accuracy: 0.913797, Val accuracy: 0.889769\n",
            "Average loss: 0.258715, Train accuracy: 0.921783, Val accuracy: 0.885469\n",
            "Average loss: 0.238319, Train accuracy: 0.926577, Val accuracy: 0.887311\n",
            "Average loss: 0.223037, Train accuracy: 0.930365, Val accuracy: 0.881578\n",
            "Average loss: 0.209081, Train accuracy: 0.934887, Val accuracy: 0.891680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u_O9qiYySvuj"
      },
      "source": [
        "# Подбор гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i6mhfdQ9K-N3",
        "colab": {}
      },
      "source": [
        "# The key hyperparameters we're going to tune are learning speed, annealing rate and regularization\n",
        "# We also encourage you to try different optimizers as well\n",
        "from itertools import product\n",
        "\n",
        "Hyperparams = namedtuple(\"Hyperparams\", ['learning_rate', 'anneal_epochs', 'reg'])\n",
        "RunResult = namedtuple(\"RunResult\", ['model', 'train_history', 'val_history', 'final_val_accuracy'])\n",
        "\n",
        "learning_rates = [1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "anneal_coeff = 0.2\n",
        "anneal_epochs = [1, 5, 10, 15, 20, 50]\n",
        "reg = [1e-3, 1e-4, 1e-5, 1e-7]\n",
        "\n",
        "batch_size = 64\n",
        "epoch_num = 10\n",
        "\n",
        "hypes = list(product(learning_rates, anneal_epochs, reg))\n",
        "# Record all the runs here\n",
        "# Key should be Hyperparams and values should be RunResult\n",
        "run_record = {} \n",
        "\n",
        "# Use grid search or random search and record all runs in run_record dictionnary \n",
        "# Important: perform search in logarithmic space!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5W1KntscTzR",
        "colab_type": "code",
        "outputId": "9ba11f72-a721-495b-da2a-2feab05ddff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "for rate, ann, re in hypes:\n",
        "    print(f\"Learing rate: {rate}; anneal epoch: {ann}; reg strength: {re}\")\n",
        "    params = Hyperparams(rate, ann, re)\n",
        "    \n",
        "    LeNet = nn.Sequential(\n",
        "                          nn.Conv2d(3, 64, 3, padding=1),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.MaxPool2d(4),\n",
        "                          nn.Conv2d(64, 64, 3, padding=1),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.MaxPool2d(4),    \n",
        "                          Flattener(),\n",
        "                          nn.Linear(64*2*2, 120),\n",
        "                          nn.Linear(120, 84),\n",
        "                          nn.Linear(84, 10))\n",
        "\n",
        "    LeNet.type(torch.cuda.FloatTensor)\n",
        "    LeNet.to(device)\n",
        "    \n",
        "    loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
        "  \n",
        "    optimizer = optim.Adam(LeNet.parameters(), lr=rate, weight_decay=re)\n",
        "    sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=ann, gamma=anneal_coeff)\n",
        "\n",
        "    loss_history, train_history, val_history = train_model(\n",
        "                                                            LeNet, \n",
        "                                                            train_aug_loader,\n",
        "                                                            val_loader,\n",
        "                                                            loss,\n",
        "                                                            optimizer,\n",
        "                                                            epoch_num,\n",
        "                                                            scheduler=sched)\n",
        "\n",
        "    results = RunResult(LeNet, train_history, val_history, val_history[-1])\n",
        "    run_record[params] = results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learing rate: 1.0; anneal epoch: 1; reg strength: 0.001\n",
            "Average loss: 818301.437500, Train accuracy: 0.106764, Val accuracy: 0.066958\n",
            "Average loss: 2582.169678, Train accuracy: 0.132427, Val accuracy: 0.094738\n",
            "Average loss: 766.556946, Train accuracy: 0.135208, Val accuracy: 0.192274\n",
            "Average loss: 251.558807, Train accuracy: 0.137068, Val accuracy: 0.115624\n",
            "Average loss: 151.438065, Train accuracy: 0.140873, Val accuracy: 0.066958\n",
            "Average loss: 130.946945, Train accuracy: 0.156708, Val accuracy: 0.148727\n",
            "Average loss: 128.810104, Train accuracy: 0.175921, Val accuracy: 0.192274\n",
            "Average loss: 128.492233, Train accuracy: 0.183599, Val accuracy: 0.192274\n",
            "Average loss: 128.454681, Train accuracy: 0.188342, Val accuracy: 0.192274\n",
            "Average loss: 128.441360, Train accuracy: 0.188445, Val accuracy: 0.192274\n",
            "Learing rate: 1.0; anneal epoch: 1; reg strength: 0.0001\n",
            "Average loss: 2344113.500000, Train accuracy: 0.106525, Val accuracy: 0.079380\n",
            "Average loss: 2767.425537, Train accuracy: 0.118776, Val accuracy: 0.094738\n",
            "Average loss: 1170.977417, Train accuracy: 0.133809, Val accuracy: 0.115624\n",
            "Average loss: 681.761597, Train accuracy: 0.140361, Val accuracy: 0.066958\n",
            "Average loss: 566.422607, Train accuracy: 0.139371, Val accuracy: 0.192274\n",
            "Average loss: 537.086609, Train accuracy: 0.150582, Val accuracy: 0.115624\n",
            "Average loss: 530.131775, Train accuracy: 0.153875, Val accuracy: 0.115624\n",
            "Average loss: 528.766418, Train accuracy: 0.170221, Val accuracy: 0.192274\n",
            "Average loss: 528.507812, Train accuracy: 0.186517, Val accuracy: 0.192274\n",
            "Average loss: 528.488037, Train accuracy: 0.188445, Val accuracy: 0.192274\n",
            "Learing rate: 1.0; anneal epoch: 1; reg strength: 1e-05\n",
            "Average loss: 10356551.000000, Train accuracy: 0.110415, Val accuracy: 0.073510\n",
            "Average loss: 8958.200195, Train accuracy: 0.129270, Val accuracy: 0.065593\n",
            "Average loss: 2753.328857, Train accuracy: 0.106423, Val accuracy: 0.148727\n",
            "Average loss: 1924.598022, Train accuracy: 0.097533, Val accuracy: 0.192274\n",
            "Average loss: 1691.956299, Train accuracy: 0.094427, Val accuracy: 0.094738\n",
            "Average loss: 1614.371948, Train accuracy: 0.095758, Val accuracy: 0.094738\n",
            "Average loss: 1590.235352, Train accuracy: 0.096236, Val accuracy: 0.094738\n",
            "Average loss: 1584.087891, Train accuracy: 0.094154, Val accuracy: 0.094738\n",
            "Average loss: 1582.978760, Train accuracy: 0.093745, Val accuracy: 0.094738\n",
            "Average loss: 1582.754761, Train accuracy: 0.093745, Val accuracy: 0.094738\n",
            "Learing rate: 1.0; anneal epoch: 1; reg strength: 1e-07\n",
            "Average loss: 831260.875000, Train accuracy: 0.103658, Val accuracy: 0.115624\n",
            "Average loss: 3632.315430, Train accuracy: 0.123912, Val accuracy: 0.094738\n",
            "Average loss: 1111.783691, Train accuracy: 0.130345, Val accuracy: 0.148727\n",
            "Average loss: 570.764404, Train accuracy: 0.129321, Val accuracy: 0.065593\n",
            "Average loss: 443.931000, Train accuracy: 0.131352, Val accuracy: 0.148727\n",
            "Average loss: 418.927307, Train accuracy: 0.139388, Val accuracy: 0.192274\n",
            "Average loss: 415.362518, Train accuracy: 0.178053, Val accuracy: 0.094738\n",
            "Average loss: 415.133759, Train accuracy: 0.187609, Val accuracy: 0.192274\n",
            "Average loss: 415.024139, Train accuracy: 0.188445, Val accuracy: 0.192274\n",
            "Average loss: 415.023407, Train accuracy: 0.188445, Val accuracy: 0.192274\n",
            "Learing rate: 1.0; anneal epoch: 5; reg strength: 0.001\n",
            "Average loss: 1239577.000000, Train accuracy: 0.111644, Val accuracy: 0.065593\n",
            "Average loss: 14815.668945, Train accuracy: 0.114801, Val accuracy: 0.061293\n",
            "Average loss: 63654612.000000, Train accuracy: 0.126421, Val accuracy: 0.073510\n",
            "Average loss: 28241.949219, Train accuracy: 0.118043, Val accuracy: 0.094738\n",
            "Average loss: 22759.500000, Train accuracy: 0.117838, Val accuracy: 0.065593\n",
            "Average loss: 3336.576660, Train accuracy: 0.113777, Val accuracy: 0.192274\n",
            "Average loss: 3118.931885, Train accuracy: 0.111337, Val accuracy: 0.079380\n",
            "Average loss: 3166.680664, Train accuracy: 0.115159, Val accuracy: 0.061293\n",
            "Average loss: 14029.709961, Train accuracy: 0.116524, Val accuracy: 0.115624\n",
            "Average loss: 11652.884766, Train accuracy: 0.109579, Val accuracy: 0.115624\n",
            "Learing rate: 1.0; anneal epoch: 5; reg strength: 0.0001\n",
            "Average loss: 1036665.375000, Train accuracy: 0.114835, Val accuracy: 0.065593\n",
            "Average loss: 8013.229980, Train accuracy: 0.113248, Val accuracy: 0.061293\n",
            "Average loss: 4648.590820, Train accuracy: 0.114510, Val accuracy: 0.073510\n",
            "Average loss: 6925701.000000, Train accuracy: 0.119066, Val accuracy: 0.065593\n",
            "Average loss: 28523509760.000000, Train accuracy: 0.128195, Val accuracy: 0.115624\n",
            "Average loss: 218553.343750, Train accuracy: 0.148927, Val accuracy: 0.061293\n",
            "Average loss: 155723.156250, Train accuracy: 0.152083, Val accuracy: 0.192274\n",
            "Average loss: 147100.765625, Train accuracy: 0.139218, Val accuracy: 0.192274\n",
            "Average loss: 130583.046875, Train accuracy: 0.139917, Val accuracy: 0.115624\n",
            "Average loss: 129699.500000, Train accuracy: 0.136215, Val accuracy: 0.115624\n",
            "Learing rate: 1.0; anneal epoch: 5; reg strength: 1e-05\n",
            "Average loss: 826660.562500, Train accuracy: 0.110671, Val accuracy: 0.148727\n",
            "Average loss: 4781.085938, Train accuracy: 0.115005, Val accuracy: 0.073510\n",
            "Average loss: 2744.929443, Train accuracy: 0.115159, Val accuracy: 0.094738\n",
            "Average loss: 1987.887451, Train accuracy: 0.115278, Val accuracy: 0.148727\n",
            "Average loss: 1477.790039, Train accuracy: 0.115876, Val accuracy: 0.094738\n",
            "Average loss: 268.854370, Train accuracy: 0.115329, Val accuracy: 0.148727\n",
            "Average loss: 300.499542, Train accuracy: 0.115142, Val accuracy: 0.094738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-aeeb65429512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for rate, ann, re in hypes:\\n    print(f\"Learing rate: {rate}; anneal epoch: {ann}; reg strength: {re}\")\\n    params = Hyperparams(rate, ann, re)\\n    \\n    LeNet = nn.Sequential(\\n                          nn.Conv2d(3, 64, 3, padding=1),\\n                          nn.ReLU(inplace=True),\\n                          nn.MaxPool2d(4),\\n                          nn.Conv2d(64, 64, 3, padding=1),\\n                          nn.ReLU(inplace=True),\\n                          nn.MaxPool2d(4),    \\n                          Flattener(),\\n                          nn.Linear(64*2*2, 120),\\n                          nn.Linear(120, 84),\\n                          nn.Linear(84, 10))\\n\\n    LeNet.type(torch.cuda.FloatTensor)\\n    LeNet.to(device)\\n    \\n    loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\\n  \\n    optimizer = optim.Adam(LeNet.parameters(), lr=rate, weight_decay=re)\\n    sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=ann, gamma=anneal_coeff)\\n\\n    loss_history, train_history, val_history = train_model(\\n                                                            LeNet, \\n                                                            train_aug_loader,\\n                                                            val_loader,\\n                                                            ...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-970daae56a0c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcorrect_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/svhn.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6xExdw8JB1l",
        "outputId": "f8dc4fd3-0eac-4c52-b35e-d9fe205e59aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "best_val_accuracy = None\n",
        "best_hyperparams = None\n",
        "best_run = None\n",
        "\n",
        "for hyperparams, run_result in run_record.items():\n",
        "    if best_val_accuracy is None or best_val_accuracy < run_result.final_val_accuracy:\n",
        "        best_val_accuracy = run_result.final_val_accuracy\n",
        "        best_hyperparams = hyperparams\n",
        "        best_run = run_result\n",
        "        \n",
        "print(\"Best validation accuracy: %4.2f, best hyperparams: %s\" % (best_val_accuracy, best_hyperparams))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best validation accuracy: 0.19, best hyperparams: Hyperparams(learning_rate=1.0, anneal_epochs=1, reg=0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LOmsR0uVgtgf"
      },
      "source": [
        "# Свободное упражнение - догоним и перегоним LeNet!\n",
        "\n",
        "Попробуйте найти архитектуру и настройки тренировки, чтобы выступить лучше наших бейзлайнов.\n",
        "\n",
        "Что можно и нужно попробовать:\n",
        "- BatchNormalization (для convolution layers он в PyTorch называется [batchnorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d))\n",
        "- Изменить количество слоев и их толщину\n",
        "- Изменять количество эпох тренировки\n",
        "- Попробовать и другие агментации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3gcYc_FjA6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEk4-QGccTza",
        "colab_type": "code",
        "outputId": "b8b47eac-6e1a-4976-faee-be1880e4831d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "lenet_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 18, 5, padding=0), \n",
        "            nn.BatchNorm2d(18),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(18, 24, 5, padding=0), \n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.MaxPool2d(2), \n",
        "            Flattener(), \n",
        "            nn.Linear(24*5*5, 200), \n",
        "            nn.BatchNorm1d(200), \n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(200, 84),  \n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(84, 10), \n",
        "          )\n",
        "\n",
        "lenet_model.type(torch.cuda.FloatTensor)\n",
        "lenet_model.to(device)\n",
        "\n",
        "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
        "optimizer = optim.SGD(lenet_model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
        "\n",
        "# Let's train it!\n",
        "loss_history, train_history, val_history = train_model(lenet_model, train_aug_loader, val_loader, loss, optimizer, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 1.394111, Train accuracy: 0.560796, Val accuracy: 0.797829\n",
            "Average loss: 0.600873, Train accuracy: 0.824472, Val accuracy: 0.852024\n",
            "Average loss: 0.471731, Train accuracy: 0.857899, Val accuracy: 0.862740\n",
            "Average loss: 0.410099, Train accuracy: 0.875815, Val accuracy: 0.862876\n",
            "Average loss: 0.375843, Train accuracy: 0.886462, Val accuracy: 0.881783\n",
            "Average loss: 0.343430, Train accuracy: 0.895420, Val accuracy: 0.886902\n",
            "Average loss: 0.320819, Train accuracy: 0.904583, Val accuracy: 0.893113\n",
            "Average loss: 0.300431, Train accuracy: 0.911016, Val accuracy: 0.892635\n",
            "Average loss: 0.282118, Train accuracy: 0.916032, Val accuracy: 0.895639\n",
            "Average loss: 0.266642, Train accuracy: 0.919599, Val accuracy: 0.895092\n",
            "Average loss: 0.253399, Train accuracy: 0.925110, Val accuracy: 0.901099\n",
            "Average loss: 0.242389, Train accuracy: 0.928847, Val accuracy: 0.899461\n",
            "Average loss: 0.229143, Train accuracy: 0.931833, Val accuracy: 0.905126\n",
            "Average loss: 0.219000, Train accuracy: 0.935570, Val accuracy: 0.905194\n",
            "Average loss: 0.207663, Train accuracy: 0.939494, Val accuracy: 0.900894\n",
            "Average loss: 0.197052, Train accuracy: 0.943112, Val accuracy: 0.905058\n",
            "Average loss: 0.188248, Train accuracy: 0.945756, Val accuracy: 0.909153\n",
            "Average loss: 0.178691, Train accuracy: 0.949698, Val accuracy: 0.908743\n",
            "Average loss: 0.170091, Train accuracy: 0.951711, Val accuracy: 0.910109\n",
            "Average loss: 0.163401, Train accuracy: 0.953196, Val accuracy: 0.908880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubeKgBcnhx7N"
      },
      "source": [
        "# Финальный аккорд - проверим лучшую модель на test set\n",
        "\n",
        "В качестве разнообразия - напишите код для прогона модели на test set вы.\n",
        "\n",
        "В результате вы должны натренировать модель, которая покажет более **90%** точности на test set.  \n",
        "Как водится, лучший результат в группе получит дополнительные баллы!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EIqM1kdeh-hd",
        "outputId": "68dd8e7e-c454-4ae1-a3bd-cc9de9f68fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO Write the code to compute accuracy on test set\n",
        "final_test_accuracy = compute_accuracy(lenet_model, train_loader)\n",
        "print(\"Final test accuracy - \", final_test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final test accuracy -  0.9709586049209978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aoZ9Dvhhx-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}