{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "#  importlib.reload(linear_classifer)\n",
    "# importlib.reload(gradient_check)\n",
    "\n",
    "# Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397949\n",
      "Epoch 1, loss: 2.330601\n",
      "Epoch 2, loss: 2.310495\n",
      "Epoch 3, loss: 2.303751\n",
      "Epoch 4, loss: 2.302882\n",
      "Epoch 5, loss: 2.302900\n",
      "Epoch 6, loss: 2.302690\n",
      "Epoch 7, loss: 2.301797\n",
      "Epoch 8, loss: 2.301930\n",
      "Epoch 9, loss: 2.301484\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdnElEQVR4nO3deXRc9X338fd3ZrTYkix7ZNkGeZHE5hjHq/CYkKVJnkKWpkBJQwngpHkSkoY0kEOeJuXJaU7C0+eUJOVJmo0QSFuKkzRlSUhJEyih5dDGMrIxNl7YvBsvwvIieZM0833+mCssy7I0ske+M3c+r3N8PHPv746/Mwc+9873/uZec3dERCS6YmEXICIio0tBLyIScQp6EZGIU9CLiEScgl5EJOISYRcw0MSJE72xsTHsMkREisqKFSted/f6wdYVXNA3NjbS1tYWdhkiIkXFzLacap1aNyIiETds0JvZNDN7yszWmdlaM7tliLGXmFmvmX2w37KPmNnLwZ+P5KtwERHJTS6tm17gNndfaWY1wAoze8Ld1/UfZGZx4E7g8X7LksCXgRbAg20fdfd9eXsHIiIypGGP6N19p7uvDB53AuuBhkGG/jnwELCn37IrgCfcvSMI9yeA95xx1SIikrMR9ejNrBGYD7QOWN4AXA18f8AmDcC2fs+3M8hOwsxuMrM2M2trb28fSUkiIjKMnIPezKrJHrHf6u4HB6z+JvAFd8+cThHufo+7t7h7S339oLODRETkNOU0vdLMysiG/FJ3f3iQIS3AT80MYCLwPjPrBXYAv9dv3FTgP86gXhERGaFcZt0YcB+w3t3vGmyMuze5e6O7NwIPAp92958DvwEuN7MJZjYBuDxYlnf7D3fzrX9/mRd2HBiNlxcRKVq5HNFfBtwIrDGzVcGy24HpAO5+96k2dPcOM7sDeDZY9FV37ziDek8pFjO+9eRLpN2Z3VA7Gv+EiEhRGjbo3f0ZwHJ9QXf/6IDnPwJ+NOLKRmhcZRmzzh1H68a9o/1PiYgUlUj9MjbVVMdz2/ZzrDcddikiIgUjYkGfpLs3w/Pb1KcXEekTqaBf1JQEUPtGRKSfSAX9+LHlzJxSQ+umUTnfKyJSlCIV9JBt36zYso+e9Gn9dktEJHKiF/TNdRzpSbNG8+lFRIAIBv0ljX19erVvREQggkFfX1PBefVVLN+kE7IiIhDBoIds+6Zt8z7SGQ+7FBGR0EUz6JuSdB7rZd1rAy+yKSJSeiIa9HUAtKp9IyISzaCfUlvJjLqxmk8vIkJEgx6y7ZtnN3eQUZ9eREpchIO+jv2He3hxd2fYpYiIhCqyQa/r3oiIZEU26Kclx9IwfgzLN6tPLyKlLbJBD9k+/fJNHbirTy8ipSvaQd+c5PWubl5t7wq7FBGR0EQ66BcF8+mX6bo3IlLCIh30jXVjmVRTwXLNpxeREhbpoDczUs11tG7aqz69iJSsSAc9ZKdZ7j54jC17D4ddiohIKCIf9IuD+fRq34hIqYp80J8/qZq6qnKW6QJnIlKiIh/0ZsaipqTuOCUiJSvyQQ/ZPv2O/UfYvk99ehEpPSUR9H3Xp1efXkRKUUkE/cwpNdSOKVP7RkRKUkkEfSxmXNKY1B2nRKQklUTQQ/YCZ5v3Hmb3waNhlyIiclaVTtA3B9enV59eREpMyQT9rHPGUV2R0I1IRKTkDBv0ZjbNzJ4ys3VmttbMbhlkzJVmttrMVplZm5m9td+6rwXbrTezvzMzy/ebyEUiHmPhjAk6oheRkpPLEX0vcJu7zwIWAzeb2awBY54E5rr7POBjwL0AZvYW4DJgDjAbuAR4R55qH7FUc5JX9nTxetexsEoQETnrhg16d9/p7iuDx53AeqBhwJguP355yCqg77EDlUA5UAGUAbvzU/rI9c2nf1ZH9SJSQkbUozezRmA+0DrIuqvNbAPwGNmjetz9d8BTwM7gz2/cff0g294UtHza2tvbR/oecvbmhlrGlMXVvhGRkpJz0JtZNfAQcKu7Hxy43t0fcfeZwFXAHcE25wNvAqaS/RbwLjN72yDb3uPuLe7eUl9ff3rvJAfliRgLZoxnmU7IikgJySnozayMbMgvdfeHhxrr7k8DzWY2EbgaWBa0drqAfwMuPcOaz0iqqY4Xd3ey/3B3mGWIiJw1ucy6MeA+YL2733WKMef3zaYxswVk+/F7ga3AO8wsEews3kG2xx+aVFMSd3h2874wyxAROWsSOYy5DLgRWGNmq4JltwPTAdz9buAaYImZ9QBHgGvd3c3sQeBdwBqyJ2Z/7e6/zPN7GJG508ZTnojRunEvvz9rcpiliIicFcMGvbs/Aww5993d7wTuHGR5GvjkaVc3CirL4sybNl4nZEWkZJTML2P7W9yUZO1rB+g82hN2KSIio64kgz7VXEfGoW2L+vQiEn0lGfQLpk8gETNdn15ESkJJBv2Y8jhzptbq+vQiUhJKMugh275Zs/0Ah7t7wy5FRGRUlW7QNyXpzTgrt+wPuxQRkVFVskG/cMYEYobaNyISeSUb9DWVZcxuqNUJWRGJvJINesi2b1Zt28/RnnTYpYiIjJoSD/o6utMZVm1Tn15Eoqukg/6SxiRmqH0jIpFW0kFfO7aMmVPG6YSsiERaSQc9ZPv0K7fuo7s3E3YpIiKjouSDfnFzkqM9GdbsUJ9eRKKp5IP+ksYkAMvUpxeRiCr5oK+rruCCSdUs1/XpRSSiSj7oAVLNSdo2d9CbVp9eRKJHQQ8saqrjUHeata8dDLsUEZG8U9CTveMU6Lo3IhJNCnpg0rhKmiZWqU8vIpGkoA+kmpIs39RBOuNhlyIiklcK+sCipiQHj/ayYZf69CISLQr6QKq5DtB1b0QkehT0gYbxY5g6YYz69CISOQr6flJNdSzf3IG7+vQiEh0K+n5STUk6DnXz8p6usEsREckbBX0/qeZgPv1GzacXkehQ0PczPTmWKeMqaVWfXkQiREHfj5mRak7Sukl9ehGJDgX9AIuakrR3HmPT64fCLkVEJC8U9AOkmrLz6TXNUkSiQkE/wHn1VUysLlefXkQiY9igN7NpZvaUma0zs7VmdssgY640s9VmtsrM2szsrf3WTTezx81sffAajfl9C/llZixqStK6ca/69CISCbkc0fcCt7n7LGAxcLOZzRow5klgrrvPAz4G3Ntv3f3A1939TcAiYM+Zlz26Uk11vHbgKNv3HQm7FBGRMzZs0Lv7TndfGTzuBNYDDQPGdPnxw98qwAGCHULC3Z/oN+5wHusfFW/Mp1f7RkQiYEQ9+qDtMh9oHWTd1Wa2AXiM7FE9wIXAfjN72MyeM7Ovm1l8kG1vClo+be3t7SN9D3l34aQaxo8t0w+nRCQScg56M6sGHgJudfeTruXr7o+4+0zgKuCOYHECeBvweeASoBn46CDb3uPuLe7eUl9fP+I3kW+xmHFJY1JH9CISCTkFvZmVkQ35pe7+8FBj3f1poNnMJgLbgVXuvtHde4GfAwvOsOazItWUZGvHYXYeUJ9eRIpbLrNuDLgPWO/ud51izPnBOMxsAVAB7AWeBcabWd9h+ruAdfkofLQtbtZ8ehGJhkQOYy4DbgTWmNmqYNntwHQAd78buAZYYmY9wBHg2uDkbNrMPg88GewIVgA/zPN7GBVvOmccNRUJlm3s4Mp5DcNvICJSoIYNend/BrBhxtwJ3HmKdU8Ac06ruhDFY0ZL4wRaN+mErIgUN/0ydgip5jo2th9iT+fRsEsRETltCvohpJqy8+mf3bQv5EpERE6fgn4IsxtqGVseV/tGRIqagn4IZfEYC2dMoHWjZt6ISPFS0A8j1ZTkxd2d7DvUHXYpIiKnRUE/jFTffPrNOqoXkeKkoB/GnKm1VCRiat+ISNFS0A+jIhFn/vTxOiErIkVLQZ+DVFMd63Ye5ODRnrBLEREZMQV9DlLNSdyhTX16ESlCCvoczJ82gbK4qU8vIkVJQZ+DMeVx5k4dzzJdyVJEipCCPkep5iQv7DjAoWO9YZciIjIiCvocpZrqSGecFVt03RsRKS4K+hwtmDGBeMw0zVJEio6CPkfVFQlmN9TqhKyIFB0F/Qgsbkry/Pb9HO1Jh12KiEjOFPQjsKgpSU/aWblVfXoRKR4K+hFoaUxihto3IlJUFPQjUDumjFnnjGO55tOLSBFR0I9QqqmOlVv3caxXfXoRKQ4K+hFa1JTkWG+G1dsPhF2KiEhOFPQjtCi4YXjrRs2nF5HioKAfoWRVORdNrqFVfXoRKRIK+tOQak6yYss+etKZsEsRERmWgv40LGpKcrg7zQs71KcXkcKnoD8Nb/Tp1b4RkSKgoD8Nk2oqaa6v0nx6ESkKCvrTlGqq49lNHaQzHnYpIiJDUtCfplRTks5jvazfeTDsUkREhqSgP02p5myffpnm04tIgVPQn6ZzascwPTlWfXoRKXjDBr2ZTTOzp8xsnZmtNbNbBhlzpZmtNrNVZtZmZm8dsH6cmW03s+/ks/iwLWpKsnxzBxn16UWkgOVyRN8L3Obus4DFwM1mNmvAmCeBue4+D/gYcO+A9XcAT59psYUm1ZRk/+EeXtrTGXYpIiKnNGzQu/tOd18ZPO4E1gMNA8Z0uXvfYW0V8MYhrpktBCYDj+er6EKxuLkOQO0bESloI+rRm1kjMB9oHWTd1Wa2AXiM7FE9ZhYD/hb4/DCve1PQ8mlrb28fSUmhmjphDOfWVupGJCJS0HIOejOrBh4CbnX3k+YUuvsj7j4TuIpsqwbg08Cv3H37UK/t7ve4e4u7t9TX1+defcjMjEVNSVo37eX4FxoRkcKSU9CbWRnZkF/q7g8PNdbdnwaazWwicCnwGTPbDHwDWGJmf3NmJReWVHMdr3d182r7obBLEREZVGK4AWZmwH3Aene/6xRjzgdedXc3swVABbDX3a/vN+ajQIu7fzEvlReIVHDdm+WbOjh/UnXI1YiInGzYoAcuA24E1pjZqmDZ7cB0AHe/G7iG7NF6D3AEuNZLpJfRNLGK+poKWjft5cOp6WGXIyJykmGD3t2fAWyYMXcCdw4z5h+AfxhBbUXhjT79xg7cnewXIBGRwqFfxubB4qYkuw4eZWvH4bBLERE5iYI+D1LBfHpdn15ECpGCPg/Or69mwtgyzacXkYKkoM+DWOz4fHoRkUKjoM+TVFMd2/cdYcf+I2GXIiJyAgV9nvRdn365jupFpMAo6PNk5pRx1FQm1KcXkYKjoM+TeMxY1JjUzBsRKTgK+jxKNSfZ9Poh9hw8GnYpIiJvUNDnUapJ8+lFpPAo6PPo4nPHUVUe1zRLESkoCvo8SsRjLGxM6oSsiBQUBX2epZqSvLyni9e7joVdiogIoKDPu3deNAkz+PKja3XXKREpCAr6PJt17ji+8J6ZPLZ6J9996pWwyxERyenGIzJCn3x7Mxt2HuQbj7/EBZNruOLiKWGXJCIlTEf0o8DM+Jtr5jB3ai2f++dVbNh10r3URUTOGgX9KKksi/ODG1uorkjw8X9so+NQd9gliUiJUtCPoim1ldyzpIU9ncf4swdW0JPOhF2SiJQgBf0omzdtPHde82ZaN3XwlV+uDbscESlBOhl7Flw9fyobdnXyg//cyEVTxnHj4hlhlyQiJURH9GfJX1wxk3deVM9XHl3L717VJRJE5OxR0J8l8ZjxrevmM6NuLJ9euoKtew+HXZKIlAgF/Vk0rrKMez9yCemM84n72+g61ht2SSJSAhT0Z1nTxCq+e/0CXmnv4nP/vIpMRpdJEJHRpaAPwdsuqOdL738TT6zbzf/795fCLkdEIk6zbkLy0bc0smFnJ9/+7StcOLmGD8w9N+ySRCSidEQfEjPjq1ddTMuMCfyvB5/nhR0Hwi5JRCJKQR+iikSc79+wkOTYcj5xfxvtnbqGvYjkn4I+ZPU1FdyzpIV9h7v51AMrONabDrskEYkYBX0BmN1Qy9/+8TxWbNnHlx55QTcsEZG8UtAXiPfPOYfPvvsC/mXFdn70X5vDLkdEImTYoDezaWb2lJmtM7O1ZnbLIGOuNLPVZrbKzNrM7K3B8nlm9rtgu9Vmdu1ovImouPXdF3DFxZP568fW8fRL7WGXIyIRkcsRfS9wm7vPAhYDN5vZrAFjngTmuvs84GPAvcHyw8ASd78YeA/wTTMbn5/SoycWM+760DwunFzDZ368ko3tXWGXJCIRMGzQu/tOd18ZPO4E1gMNA8Z0+fHGchXgwfKX3P3l4PFrwB6gPn/lR09VRYIfLmkhEY/x8fvbOHi0J+ySRKTIjahHb2aNwHygdZB1V5vZBuAxskf1A9cvAsqBVwdZd1PQ8mlrb1fLYlpyLN+7fgFb9x7msz95jrQukyAiZyDnoDezauAh4FZ3P+kmqO7+iLvPBK4C7hiw7TnAPwF/6u4n3WbJ3e9x9xZ3b6mv1wE/wOLmOr5y5cX8x4vtfO3XG8IuR0SKWE6XQDCzMrIhv9TdHx5qrLs/bWbNZjbR3V83s3Fkj/L/t7svO/OSS8f1qRls2NnJD57eyIWTa7hm4dSwSxKRIpTLrBsD7gPWu/tdpxhzfjAOM1sAVAB7zawceAS4390fzF/ZpeOvPjCLS5vr+MuH17By676wyxGRIpRL6+Yy4EbgXcH0yVVm9j4z+5SZfSoYcw3wgpmtAr4LXBucnP0Q8Hbgo/22nTcabySqyuIxvnf9AibXVvDJf1rBrgNHwy5JRIqMFdqvMFtaWrytrS3sMgrOi7s6+aPv/RfnTarmZ5+8lMqyeNgliUgBMbMV7t4y2Dr9MrZIXDSlhm/+yXzW7DjAFx5arcskiEjOFPRF5PdnTebzl1/EL1a9xt3/uTHsckSkSCjoi8ynf+88/mDOOXztNxt4cv3usMsRkSKgoC8yZsbXPziX2efWcstPV/Hy7s6wSxKRAqegL0JjyuPcs2QhlWVxPn5/G/sOdYddkogUMAV9kTqndgw/uHEhO/cf5eYfr6QnfdIPjkVEAAV9UVs4YwL/94/ezH+/upe/fmx92OWISIHK6RIIUrg+uHAqG3Ye5N5nNnHRlBquWzQ97JJEpMDoiD4Cvvjembz9wnr+6hcvsHxTR9jliEiBUdBHQCIe49vXzWfahLH82QMr2L7vcNgliUgBUdBHRO2YMn74kRa60xk+cf8KDnf3hl2SiBQIBX2EnFdfzXc+vIAXdx3ktp89T0Y3LBERFPSR844L67n9fW/i317YxV1PvKSwFxHNuomi//nWJjbs6uQ7T73CL57fwYcXzeCPW6Yysboi7NJEJAS6THFEpTPOr9bs5IFlW2jd1EFZ3Hjv7HO4YfEMLmmcQHCfGBGJiKEuU6ygLwEv7+5kaetWHlq5nc6jvVw4uZrrUzO4ekED4yrLwi5PRPJAQS8AHO7u5V+f38kDrVtYvf0AY8riXDnvXG5YPIPZDbVhlyciZ0BBLydZvX0/S5dt5RfP7+BoT4a5U2u5fvEMPjDnXMaU6+5VIsVGQS+ndOBID4+s3M4DrVt5ZU8X4yoTXLNwKtenZnD+pOqwyxORHCnoZVjuTuumDpa2buXXL+ykJ+0sbk5yw+IZXD5rCuUJzcQVKWQKehmR9s5j/MuKbfy4dSvb9x1hYnUF114ylesWTWfqhLFhlycig1DQy2lJZ5ynX2pnaesWfrthDw6886JJ3LB4Ou+4cBLxmKZoihQKBb2csR37j/CT1q389NltvN51jIbxY/hwajofaplGfY1+iCUSNgW95E1POsPja3eztHUL//3qXsrixuUXT+GG1AwWNyf1QyyRkAwV9LoEgoxIWTzG++ecw/vnnMOr7V0sXbaVB1ds47HVOzmvvorrUzO4ZuFUasfoh1gihUJH9HLGjvak+eXzr7G0dSurtu2nsizGH849l+tTM5g7bXzY5YmUBLVu5Kx5YccBlrZu4efPvcaRnjRvbqjliosnUzu2nOqKONUVZVRVxKkJ/q6uSFBdmWBMWVxtH5EzoKCXs+7g0R5+/twOli7byou7O4cdHzOoqkhkg78iQVVFgprKBFXl2R1B/+XZ54PsNIJx2mlIKVKPXs66cZVlLLm0kSWXNnK4u5euY70cOpam62j2cfZ5L53B3wOX9z3edeDoCc9zubx+306j5oQdQ4Ky+PEffQ08wBn4sv1Xn7xu6CIGrvYBrxAzI2ZGPJb9OxELHseMuEE8FiMe44T12XXZcX1/YgOexy077sTxx1+vb3z234tRVR4/YSeqb1bRpaCXUTe2PMHY8gTUnNnruDtHezJ0Hus5rZ1Gb/rEwB2YZyfFW78BA9cNt60NsW3GnXTGSbuTzkAm4/RmMmQ8+9uFN/64k+kbl+4b72Tc6c34STuUfIgZVFckqKksC75BxamuLKOm386guu/bVr9lNRUnf/Pqv2OVcCnopWiYGWPK49mLrp3hTiMK/IQdRrATyEDagx1H8Di7Izm+k0hnnN60c6g7u1M81N1LZ9/OMfg7+7yHrmO9HDjSw459h99Yf6g7nVN9lWUxqivKqKlMHN9p9HteXZlgbFkcJ7vzy3j2PfU9znh2Z5bJHH+e6bfePft+h9zW+97z0OsByhNxyuMxKhIxyhMxyuPB34kYZcHjigHLT3iciFEx4PmgY+MxEmd5J6igFylSZkYibmf9f+J0pt9OIvgm1dVvR3H8eQ9dx9LB8uxOY3vfDiMY05s5+VtWtrWVfX/x4HHMLLsuaFn1rY/Z8VZYLHb8cf/XyT7vNzbWf7vg25dnL/DX3ZuhuzdNdzoTPA7+pDP0pPP3FSpm9Av/+Bs7l9kNtXz7uvl5+3f6DPvfiJlNA+4HJpNtV97j7t8aMOZK4A4gA/QCt7r7M8G6jwBfCob+H3f/x/yVLyJnWzxmjKssO+Ob1rg7PWk/IcgL+fxAJuP0ZE4M/77HxwY8P2n9gHU96ePrj/UbP23CmFGpPZeDgV7gNndfaWY1wAoze8Ld1/Ub8yTwqLu7mc0BfgbMNLMk8GWghexOYoWZPeru+/L8PkSkyJgZ5YnCDfaBYjGjIhanIlF892sYtlHk7jvdfWXwuBNYDzQMGNPlx6ciVHF8osIVwBPu3hGE+xPAe/JVvIiIDG9EZwTMrBGYD7QOsu5qM9sAPAZ8LFjcAGzrN2w7A3YSwbY3mVmbmbW1t7ePpCQRERlGzkFvZtXAQ2T77wcHrnf3R9x9JnAV2X59ztz9HndvcfeW+vr6kWwqIiLDyCnozayMbMgvdfeHhxrr7k8DzWY2EdgBTOu3emqwTEREzpJhg96yp8HvA9a7+12nGHN+MA4zWwBUAHuB3wCXm9kEM5sAXB4sExGRsySXWTeXATcCa8xsVbDsdmA6gLvfDVwDLDGzHuAIcG1wcrbDzO4Ang22+6q7d+TzDYiIyNB0UTMRkQgY6qJmuhiFiEjEFdwRvZm1A1vO4CUmAq/nqZxip8/iRPo8TqTP47gofBYz3H3QaYsFF/RnyszaTvX1pdTosziRPo8T6fM4LuqfhVo3IiIRp6AXEYm4KAb9PWEXUED0WZxIn8eJ9HkcF+nPInI9ehEROVEUj+hFRKQfBb2ISMRFJujN7D1m9qKZvWJmXwy7njCZ2TQze8rM1pnZWjO7JeyawmZmcTN7zsz+NexawmZm483sQTPbYGbrzezSsGsKk5l9Lvj/5AUz+4mZVYZdU75FIujNLA58F3gvMAu4zsxmhVtVqPruCjYLWAzcXOKfB8AtZG+aI/At4NfBZcXnUsKfi5k1AJ8FWtx9NhAH/iTcqvIvEkEPLAJecfeN7t4N/BS4MuSaQpPLXcFKiZlNBd4P3Bt2LWEzs1rg7WSvSIu7d7v7/nCrCl0CGGNmCWAs8FrI9eRdVII+pztZlaKh7gpWQr4J/AXZm9eXuiagHfj7oJV1r5lVhV1UWNx9B/ANYCuwEzjg7o+HW1X+RSXoZRDD3RWsFJjZHwB73H1F2LUUiASwAPi+u88HDgEle04ruE/GlWR3gOcCVWZ2Q7hV5V9Ugl53shpgJHcFi7jLgD80s81kW3rvMrMHwi0pVNuB7e7e9w3vQbLBX6r+B7DJ3dvdvQd4GHhLyDXlXVSC/lngAjNrMrNysidTHg25ptDkclewUuHuf+nuU929kex/F79198gdseXK3XcB28zsomDRu4F1IZYUtq3AYjMbG/x/824ieHI6lztMFTx37zWzz5C9TWEc+JG7rw25rDANelcwd/9ViDVJ4fhzYGlwULQR+NOQ6wmNu7ea2YPASrKz1Z4jgpdD0CUQREQiLiqtGxEROQUFvYhIxCnoRUQiTkEvIhJxCnoRkYhT0IuIRJyCXkQk4v4/E90kOIV0A5QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.124\n",
      "Epoch 0, loss: 2.396122\n",
      "Epoch 1, loss: 2.330286\n",
      "Epoch 2, loss: 2.311307\n",
      "Epoch 3, loss: 2.304966\n",
      "Epoch 4, loss: 2.302548\n",
      "Epoch 5, loss: 2.302356\n",
      "Epoch 6, loss: 2.301226\n",
      "Epoch 7, loss: 2.302575\n",
      "Epoch 8, loss: 2.301648\n",
      "Epoch 9, loss: 2.300996\n",
      "Epoch 10, loss: 2.301319\n",
      "Epoch 11, loss: 2.301630\n",
      "Epoch 12, loss: 2.302040\n",
      "Epoch 13, loss: 2.302776\n",
      "Epoch 14, loss: 2.301997\n",
      "Epoch 15, loss: 2.301170\n",
      "Epoch 16, loss: 2.301850\n",
      "Epoch 17, loss: 2.301307\n",
      "Epoch 18, loss: 2.302009\n",
      "Epoch 19, loss: 2.301612\n",
      "Epoch 20, loss: 2.301402\n",
      "Epoch 21, loss: 2.301722\n",
      "Epoch 22, loss: 2.302269\n",
      "Epoch 23, loss: 2.302016\n",
      "Epoch 24, loss: 2.301948\n",
      "Epoch 25, loss: 2.302136\n",
      "Epoch 26, loss: 2.301457\n",
      "Epoch 27, loss: 2.301395\n",
      "Epoch 28, loss: 2.302324\n",
      "Epoch 29, loss: 2.302314\n",
      "Epoch 30, loss: 2.301831\n",
      "Epoch 31, loss: 2.302010\n",
      "Epoch 32, loss: 2.301606\n",
      "Epoch 33, loss: 2.301418\n",
      "Epoch 34, loss: 2.301640\n",
      "Epoch 35, loss: 2.302292\n",
      "Epoch 36, loss: 2.300953\n",
      "Epoch 37, loss: 2.302985\n",
      "Epoch 38, loss: 2.301831\n",
      "Epoch 39, loss: 2.301836\n",
      "Epoch 40, loss: 2.302415\n",
      "Epoch 41, loss: 2.301963\n",
      "Epoch 42, loss: 2.301878\n",
      "Epoch 43, loss: 2.302989\n",
      "Epoch 44, loss: 2.301542\n",
      "Epoch 45, loss: 2.302269\n",
      "Epoch 46, loss: 2.301220\n",
      "Epoch 47, loss: 2.301678\n",
      "Epoch 48, loss: 2.303474\n",
      "Epoch 49, loss: 2.303108\n",
      "Epoch 50, loss: 2.302249\n",
      "Epoch 51, loss: 2.302000\n",
      "Epoch 52, loss: 2.301656\n",
      "Epoch 53, loss: 2.301274\n",
      "Epoch 54, loss: 2.300622\n",
      "Epoch 55, loss: 2.303385\n",
      "Epoch 56, loss: 2.301605\n",
      "Epoch 57, loss: 2.302240\n",
      "Epoch 58, loss: 2.302140\n",
      "Epoch 59, loss: 2.301987\n",
      "Epoch 60, loss: 2.302395\n",
      "Epoch 61, loss: 2.302062\n",
      "Epoch 62, loss: 2.302288\n",
      "Epoch 63, loss: 2.301318\n",
      "Epoch 64, loss: 2.302721\n",
      "Epoch 65, loss: 2.301329\n",
      "Epoch 66, loss: 2.301717\n",
      "Epoch 67, loss: 2.301579\n",
      "Epoch 68, loss: 2.301526\n",
      "Epoch 69, loss: 2.302562\n",
      "Epoch 70, loss: 2.301621\n",
      "Epoch 71, loss: 2.302139\n",
      "Epoch 72, loss: 2.302353\n",
      "Epoch 73, loss: 2.302302\n",
      "Epoch 74, loss: 2.302456\n",
      "Epoch 75, loss: 2.301222\n",
      "Epoch 76, loss: 2.301744\n",
      "Epoch 77, loss: 2.301678\n",
      "Epoch 78, loss: 2.302230\n",
      "Epoch 79, loss: 2.302192\n",
      "Epoch 80, loss: 2.302017\n",
      "Epoch 81, loss: 2.302115\n",
      "Epoch 82, loss: 2.302599\n",
      "Epoch 83, loss: 2.301665\n",
      "Epoch 84, loss: 2.301994\n",
      "Epoch 85, loss: 2.301416\n",
      "Epoch 86, loss: 2.301690\n",
      "Epoch 87, loss: 2.301357\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-72d5fc69146b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Now, let's train more and see if it performs better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/robez/D2CAF704CAF6E41F/Uni/CSC/sem_2/DL/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, learning_rate, reg, epochs)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mb_idxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_dW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/robez/D2CAF704CAF6E41F/Uni/CSC/sem_2/DL/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mlinear_softmax\u001b[0;34m(X, W, target_index)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============Loop # 1...============\n",
      "Epoch 0, loss: 2.301482\n",
      "Epoch 1, loss: 2.301572\n",
      "Epoch 2, loss: 2.300610\n",
      "Epoch 3, loss: 2.299273\n",
      "Epoch 4, loss: 2.296792\n",
      "Epoch 5, loss: 2.298610\n",
      "Epoch 6, loss: 2.294408\n",
      "Epoch 7, loss: 2.298996\n",
      "Epoch 8, loss: 2.291414\n",
      "Epoch 9, loss: 2.293284\n",
      "Epoch 10, loss: 2.293592\n",
      "Epoch 11, loss: 2.291504\n",
      "Epoch 12, loss: 2.290160\n",
      "Epoch 13, loss: 2.288117\n",
      "Epoch 14, loss: 2.287999\n",
      "Epoch 15, loss: 2.295090\n",
      "Epoch 16, loss: 2.290149\n",
      "Epoch 17, loss: 2.287394\n",
      "Epoch 18, loss: 2.288727\n",
      "Epoch 19, loss: 2.286893\n",
      "Epoch 20, loss: 2.289634\n",
      "Epoch 21, loss: 2.286458\n",
      "Epoch 22, loss: 2.288541\n",
      "Epoch 23, loss: 2.287537\n",
      "Epoch 24, loss: 2.281266\n",
      "Epoch 25, loss: 2.277037\n",
      "Epoch 26, loss: 2.277144\n",
      "Epoch 27, loss: 2.276771\n",
      "Epoch 28, loss: 2.277471\n",
      "Epoch 29, loss: 2.282289\n",
      "Epoch 30, loss: 2.272724\n",
      "Epoch 31, loss: 2.281312\n",
      "Epoch 32, loss: 2.277790\n",
      "Epoch 33, loss: 2.276153\n",
      "Epoch 34, loss: 2.276949\n",
      "Epoch 35, loss: 2.270842\n",
      "Epoch 36, loss: 2.273308\n",
      "Epoch 37, loss: 2.271390\n",
      "Epoch 38, loss: 2.271003\n",
      "Epoch 39, loss: 2.272159\n",
      "Epoch 40, loss: 2.262078\n",
      "Epoch 41, loss: 2.275515\n",
      "Epoch 42, loss: 2.271321\n",
      "Epoch 43, loss: 2.268225\n",
      "Epoch 44, loss: 2.274816\n",
      "Epoch 45, loss: 2.261399\n",
      "Epoch 46, loss: 2.256409\n",
      "Epoch 47, loss: 2.271121\n",
      "Epoch 48, loss: 2.268781\n",
      "Epoch 49, loss: 2.263325\n",
      "Epoch 50, loss: 2.270121\n",
      "Epoch 51, loss: 2.261850\n",
      "Epoch 52, loss: 2.269298\n",
      "Epoch 53, loss: 2.259989\n",
      "Epoch 54, loss: 2.264033\n",
      "Epoch 55, loss: 2.260800\n",
      "Epoch 56, loss: 2.275187\n",
      "Epoch 57, loss: 2.248035\n",
      "Epoch 58, loss: 2.266397\n",
      "Epoch 59, loss: 2.264515\n",
      "Epoch 60, loss: 2.259424\n",
      "Epoch 61, loss: 2.246823\n",
      "Epoch 62, loss: 2.266009\n",
      "Epoch 63, loss: 2.271867\n",
      "Epoch 64, loss: 2.263452\n",
      "Epoch 65, loss: 2.263578\n",
      "Epoch 66, loss: 2.248890\n",
      "Epoch 67, loss: 2.254622\n",
      "Epoch 68, loss: 2.250811\n",
      "Epoch 69, loss: 2.243499\n",
      "Epoch 70, loss: 2.258525\n",
      "Epoch 71, loss: 2.253661\n",
      "Epoch 72, loss: 2.258313\n",
      "Epoch 73, loss: 2.263229\n",
      "Epoch 74, loss: 2.268494\n",
      "Epoch 75, loss: 2.250369\n",
      "Epoch 76, loss: 2.239838\n",
      "Epoch 77, loss: 2.252995\n",
      "Epoch 78, loss: 2.258479\n",
      "Epoch 79, loss: 2.252958\n",
      "Epoch 80, loss: 2.260049\n",
      "Epoch 81, loss: 2.250423\n",
      "Epoch 82, loss: 2.245865\n",
      "Epoch 83, loss: 2.251529\n",
      "Epoch 84, loss: 2.250626\n",
      "Epoch 85, loss: 2.239320\n",
      "Epoch 86, loss: 2.238613\n",
      "Epoch 87, loss: 2.211674\n",
      "Epoch 88, loss: 2.234844\n",
      "Epoch 89, loss: 2.226016\n",
      "Epoch 90, loss: 2.243467\n",
      "Epoch 91, loss: 2.233254\n",
      "Epoch 92, loss: 2.249254\n",
      "Epoch 93, loss: 2.227708\n",
      "Epoch 94, loss: 2.247818\n",
      "Epoch 95, loss: 2.244775\n",
      "Epoch 96, loss: 2.224841\n",
      "Epoch 97, loss: 2.227721\n",
      "Epoch 98, loss: 2.226550\n",
      "Epoch 99, loss: 2.229830\n",
      "Epoch 100, loss: 2.228885\n",
      "Epoch 101, loss: 2.239893\n",
      "Epoch 102, loss: 2.233232\n",
      "Epoch 103, loss: 2.233768\n",
      "Epoch 104, loss: 2.224033\n",
      "Epoch 105, loss: 2.227081\n",
      "Epoch 106, loss: 2.251742\n",
      "Epoch 107, loss: 2.220119\n",
      "Epoch 108, loss: 2.230299\n",
      "Epoch 109, loss: 2.218196\n",
      "Epoch 110, loss: 2.236482\n",
      "Epoch 111, loss: 2.234851\n",
      "Epoch 112, loss: 2.243287\n",
      "Epoch 113, loss: 2.242765\n",
      "Epoch 114, loss: 2.227679\n",
      "Epoch 115, loss: 2.240699\n",
      "Epoch 116, loss: 2.219928\n",
      "Epoch 117, loss: 2.229943\n",
      "Epoch 118, loss: 2.237307\n",
      "Epoch 119, loss: 2.218651\n",
      "Epoch 120, loss: 2.234145\n",
      "Epoch 121, loss: 2.225339\n",
      "Epoch 122, loss: 2.208819\n",
      "Epoch 123, loss: 2.206595\n",
      "Epoch 124, loss: 2.235945\n",
      "Epoch 125, loss: 2.236607\n",
      "Epoch 126, loss: 2.236153\n",
      "Epoch 127, loss: 2.228613\n",
      "Epoch 128, loss: 2.227754\n",
      "Epoch 129, loss: 2.238630\n",
      "Epoch 130, loss: 2.210592\n",
      "Epoch 131, loss: 2.216989\n",
      "Epoch 132, loss: 2.233788\n",
      "Epoch 133, loss: 2.215692\n",
      "Epoch 134, loss: 2.211286\n",
      "Epoch 135, loss: 2.215993\n",
      "Epoch 136, loss: 2.241655\n",
      "Epoch 137, loss: 2.202377\n",
      "Epoch 138, loss: 2.227908\n",
      "Epoch 139, loss: 2.179135\n",
      "Epoch 140, loss: 2.227932\n",
      "Epoch 141, loss: 2.239794\n",
      "Epoch 142, loss: 2.225996\n",
      "Epoch 143, loss: 2.238212\n",
      "Epoch 144, loss: 2.228090\n",
      "Epoch 145, loss: 2.240310\n",
      "Epoch 146, loss: 2.216595\n",
      "Epoch 147, loss: 2.232159\n",
      "Epoch 148, loss: 2.233862\n",
      "Epoch 149, loss: 2.217137\n",
      "Epoch 150, loss: 2.200477\n",
      "Epoch 151, loss: 2.208086\n",
      "Epoch 152, loss: 2.213718\n",
      "Epoch 153, loss: 2.211828\n",
      "Epoch 154, loss: 2.227269\n",
      "Epoch 155, loss: 2.196120\n",
      "Epoch 156, loss: 2.211507\n",
      "Epoch 157, loss: 2.226324\n",
      "Epoch 158, loss: 2.220025\n",
      "Epoch 159, loss: 2.230882\n",
      "Epoch 160, loss: 2.200413\n",
      "Epoch 161, loss: 2.193345\n",
      "Epoch 162, loss: 2.214464\n",
      "Epoch 163, loss: 2.212322\n",
      "Epoch 164, loss: 2.210361\n",
      "Epoch 165, loss: 2.225718\n",
      "Epoch 166, loss: 2.209253\n",
      "Epoch 167, loss: 2.235123\n",
      "Epoch 168, loss: 2.205953\n",
      "Epoch 169, loss: 2.204817\n",
      "Epoch 170, loss: 2.235543\n",
      "Epoch 171, loss: 2.211734\n",
      "Epoch 172, loss: 2.189674\n",
      "Epoch 173, loss: 2.204887\n",
      "Epoch 174, loss: 2.209789\n",
      "Epoch 175, loss: 2.195542\n",
      "Epoch 176, loss: 2.208864\n",
      "Epoch 177, loss: 2.197613\n",
      "Epoch 178, loss: 2.197138\n",
      "Epoch 179, loss: 2.215088\n",
      "Epoch 180, loss: 2.225601\n",
      "Epoch 181, loss: 2.221778\n",
      "Epoch 182, loss: 2.183559\n",
      "Epoch 183, loss: 2.207634\n",
      "Epoch 184, loss: 2.203075\n",
      "Epoch 185, loss: 2.219613\n",
      "Epoch 186, loss: 2.202523\n",
      "Epoch 187, loss: 2.211053\n",
      "Epoch 188, loss: 2.186805\n",
      "Epoch 189, loss: 2.223495\n",
      "Epoch 190, loss: 2.179714\n",
      "Epoch 191, loss: 2.186957\n",
      "Epoch 192, loss: 2.204688\n",
      "Epoch 193, loss: 2.224233\n",
      "Epoch 194, loss: 2.224129\n",
      "Epoch 195, loss: 2.184762\n",
      "Epoch 196, loss: 2.194909\n",
      "Epoch 197, loss: 2.235469\n",
      "Epoch 198, loss: 2.191857\n",
      "Epoch 199, loss: 2.176094\n",
      "==========\n",
      "learning rate = 0.001 reg = 0.0001 accuracy = 0.228 \n",
      "==========\n",
      "Epoch 0, loss: 2.300935\n",
      "Epoch 1, loss: 2.301429\n",
      "Epoch 2, loss: 2.299138\n",
      "Epoch 3, loss: 2.299163\n",
      "Epoch 4, loss: 2.300994\n",
      "Epoch 5, loss: 2.294395\n",
      "Epoch 6, loss: 2.293334\n",
      "Epoch 7, loss: 2.294442\n",
      "Epoch 8, loss: 2.293592\n",
      "Epoch 9, loss: 2.292478\n",
      "Epoch 10, loss: 2.297332\n",
      "Epoch 11, loss: 2.289438\n",
      "Epoch 12, loss: 2.293345\n",
      "Epoch 13, loss: 2.289046\n",
      "Epoch 14, loss: 2.288386\n",
      "Epoch 15, loss: 2.287378\n",
      "Epoch 16, loss: 2.286569\n",
      "Epoch 17, loss: 2.290121\n",
      "Epoch 18, loss: 2.282241\n",
      "Epoch 19, loss: 2.284568\n",
      "Epoch 20, loss: 2.280605\n",
      "Epoch 21, loss: 2.291517\n",
      "Epoch 22, loss: 2.282909\n",
      "Epoch 23, loss: 2.282129\n",
      "Epoch 24, loss: 2.282642\n",
      "Epoch 25, loss: 2.278654\n",
      "Epoch 26, loss: 2.279237\n",
      "Epoch 27, loss: 2.282698\n",
      "Epoch 28, loss: 2.277791\n",
      "Epoch 29, loss: 2.277123\n",
      "Epoch 30, loss: 2.279847\n",
      "Epoch 31, loss: 2.268796\n",
      "Epoch 32, loss: 2.274382\n",
      "Epoch 33, loss: 2.269392\n",
      "Epoch 34, loss: 2.269450\n",
      "Epoch 35, loss: 2.281451\n",
      "Epoch 36, loss: 2.271360\n",
      "Epoch 37, loss: 2.264061\n",
      "Epoch 38, loss: 2.279039\n",
      "Epoch 39, loss: 2.269796\n",
      "Epoch 40, loss: 2.268058\n",
      "Epoch 41, loss: 2.259836\n",
      "Epoch 42, loss: 2.275096\n",
      "Epoch 43, loss: 2.277178\n",
      "Epoch 44, loss: 2.264468\n",
      "Epoch 45, loss: 2.267848\n",
      "Epoch 46, loss: 2.263637\n",
      "Epoch 47, loss: 2.269956\n",
      "Epoch 48, loss: 2.265992\n",
      "Epoch 49, loss: 2.261671\n",
      "Epoch 50, loss: 2.271688\n",
      "Epoch 51, loss: 2.258500\n",
      "Epoch 52, loss: 2.265920\n",
      "Epoch 53, loss: 2.260482\n",
      "Epoch 54, loss: 2.248601\n",
      "Epoch 55, loss: 2.250594\n",
      "Epoch 56, loss: 2.263853\n",
      "Epoch 57, loss: 2.245038\n",
      "Epoch 58, loss: 2.261647\n",
      "Epoch 59, loss: 2.241096\n",
      "Epoch 60, loss: 2.258270\n",
      "Epoch 61, loss: 2.261407\n",
      "Epoch 62, loss: 2.248431\n",
      "Epoch 63, loss: 2.256706\n",
      "Epoch 64, loss: 2.251589\n",
      "Epoch 65, loss: 2.254621\n",
      "Epoch 66, loss: 2.260425\n",
      "Epoch 67, loss: 2.250310\n",
      "Epoch 68, loss: 2.243157\n",
      "Epoch 69, loss: 2.256012\n",
      "Epoch 70, loss: 2.266463\n",
      "Epoch 71, loss: 2.248204\n",
      "Epoch 72, loss: 2.257908\n",
      "Epoch 73, loss: 2.256824\n",
      "Epoch 74, loss: 2.244413\n",
      "Epoch 75, loss: 2.253018\n",
      "Epoch 76, loss: 2.256639\n",
      "Epoch 77, loss: 2.242572\n",
      "Epoch 78, loss: 2.262711\n",
      "Epoch 79, loss: 2.219892\n",
      "Epoch 80, loss: 2.247076\n",
      "Epoch 81, loss: 2.268861\n",
      "Epoch 82, loss: 2.230268\n",
      "Epoch 83, loss: 2.244700\n",
      "Epoch 84, loss: 2.248599\n",
      "Epoch 85, loss: 2.253703\n",
      "Epoch 86, loss: 2.229297\n",
      "Epoch 87, loss: 2.263014\n",
      "Epoch 88, loss: 2.231774\n",
      "Epoch 89, loss: 2.242221\n",
      "Epoch 90, loss: 2.248986\n",
      "Epoch 91, loss: 2.243432\n",
      "Epoch 92, loss: 2.244490\n",
      "Epoch 93, loss: 2.254638\n",
      "Epoch 94, loss: 2.224238\n",
      "Epoch 95, loss: 2.247238\n",
      "Epoch 96, loss: 2.244731\n",
      "Epoch 97, loss: 2.231099\n",
      "Epoch 98, loss: 2.226347\n",
      "Epoch 99, loss: 2.236350\n",
      "Epoch 100, loss: 2.240965\n",
      "Epoch 101, loss: 2.230298\n",
      "Epoch 102, loss: 2.227708\n",
      "Epoch 103, loss: 2.216832\n",
      "Epoch 104, loss: 2.218043\n",
      "Epoch 105, loss: 2.232015\n",
      "Epoch 106, loss: 2.227968\n",
      "Epoch 107, loss: 2.229733\n",
      "Epoch 108, loss: 2.215279\n",
      "Epoch 109, loss: 2.245371\n",
      "Epoch 110, loss: 2.208978\n",
      "Epoch 111, loss: 2.235093\n",
      "Epoch 112, loss: 2.229347\n",
      "Epoch 113, loss: 2.248248\n",
      "Epoch 114, loss: 2.225990\n",
      "Epoch 115, loss: 2.253891\n",
      "Epoch 116, loss: 2.238337\n",
      "Epoch 117, loss: 2.245342\n",
      "Epoch 118, loss: 2.231394\n",
      "Epoch 119, loss: 2.210746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, loss: 2.238124\n",
      "Epoch 121, loss: 2.234903\n",
      "Epoch 122, loss: 2.233419\n",
      "Epoch 123, loss: 2.236929\n",
      "Epoch 124, loss: 2.247923\n",
      "Epoch 125, loss: 2.221034\n",
      "Epoch 126, loss: 2.232182\n",
      "Epoch 127, loss: 2.207967\n",
      "Epoch 128, loss: 2.225558\n",
      "Epoch 129, loss: 2.227546\n",
      "Epoch 130, loss: 2.238273\n",
      "Epoch 131, loss: 2.229715\n",
      "Epoch 132, loss: 2.222445\n",
      "Epoch 133, loss: 2.221101\n",
      "Epoch 134, loss: 2.233040\n",
      "Epoch 135, loss: 2.224355\n",
      "Epoch 136, loss: 2.212910\n",
      "Epoch 137, loss: 2.248458\n",
      "Epoch 138, loss: 2.225313\n",
      "Epoch 139, loss: 2.205600\n",
      "Epoch 140, loss: 2.223047\n",
      "Epoch 141, loss: 2.212619\n",
      "Epoch 142, loss: 2.226773\n",
      "Epoch 143, loss: 2.213718\n",
      "Epoch 144, loss: 2.227161\n",
      "Epoch 145, loss: 2.211595\n",
      "Epoch 146, loss: 2.203256\n",
      "Epoch 147, loss: 2.234395\n",
      "Epoch 148, loss: 2.216009\n",
      "Epoch 149, loss: 2.212031\n",
      "Epoch 150, loss: 2.231035\n",
      "Epoch 151, loss: 2.206870\n",
      "Epoch 152, loss: 2.214467\n",
      "Epoch 153, loss: 2.226063\n",
      "Epoch 154, loss: 2.205568\n",
      "Epoch 155, loss: 2.237283\n",
      "Epoch 156, loss: 2.225876\n",
      "Epoch 157, loss: 2.218051\n",
      "Epoch 158, loss: 2.215621\n",
      "Epoch 159, loss: 2.214840\n",
      "Epoch 160, loss: 2.207384\n",
      "Epoch 161, loss: 2.203948\n",
      "Epoch 162, loss: 2.188786\n",
      "Epoch 163, loss: 2.206819\n",
      "Epoch 164, loss: 2.205030\n",
      "Epoch 165, loss: 2.219470\n",
      "Epoch 166, loss: 2.226573\n",
      "Epoch 167, loss: 2.209389\n",
      "Epoch 168, loss: 2.178725\n",
      "Epoch 169, loss: 2.211896\n",
      "Epoch 170, loss: 2.207556\n",
      "Epoch 171, loss: 2.185133\n",
      "Epoch 172, loss: 2.207735\n",
      "Epoch 173, loss: 2.197526\n",
      "Epoch 174, loss: 2.207875\n",
      "Epoch 175, loss: 2.205934\n",
      "Epoch 176, loss: 2.197714\n",
      "Epoch 177, loss: 2.212381\n",
      "Epoch 178, loss: 2.190530\n",
      "Epoch 179, loss: 2.177767\n",
      "Epoch 180, loss: 2.201605\n",
      "Epoch 181, loss: 2.201577\n",
      "Epoch 182, loss: 2.188625\n",
      "Epoch 183, loss: 2.222960\n",
      "Epoch 184, loss: 2.200383\n",
      "Epoch 185, loss: 2.224187\n",
      "Epoch 186, loss: 2.222045\n",
      "Epoch 187, loss: 2.235396\n",
      "Epoch 188, loss: 2.190041\n",
      "Epoch 189, loss: 2.191129\n",
      "Epoch 190, loss: 2.209383\n",
      "Epoch 191, loss: 2.224110\n",
      "Epoch 192, loss: 2.196655\n",
      "Epoch 193, loss: 2.203145\n",
      "Epoch 194, loss: 2.208310\n",
      "Epoch 195, loss: 2.231026\n",
      "Epoch 196, loss: 2.211434\n",
      "Epoch 197, loss: 2.190444\n",
      "Epoch 198, loss: 2.181927\n",
      "Epoch 199, loss: 2.187596\n",
      "==========\n",
      "learning rate = 0.001 reg = 1e-05 accuracy = 0.228 \n",
      "==========\n",
      "Epoch 0, loss: 2.301921\n",
      "Epoch 1, loss: 2.301173\n",
      "Epoch 2, loss: 2.301622\n",
      "Epoch 3, loss: 2.296335\n",
      "Epoch 4, loss: 2.299319\n",
      "Epoch 5, loss: 2.298948\n",
      "Epoch 6, loss: 2.297229\n",
      "Epoch 7, loss: 2.299534\n",
      "Epoch 8, loss: 2.296341\n",
      "Epoch 9, loss: 2.289850\n",
      "Epoch 10, loss: 2.292289\n",
      "Epoch 11, loss: 2.292546\n",
      "Epoch 12, loss: 2.290022\n",
      "Epoch 13, loss: 2.290762\n",
      "Epoch 14, loss: 2.282364\n",
      "Epoch 15, loss: 2.287957\n",
      "Epoch 16, loss: 2.285490\n",
      "Epoch 17, loss: 2.284623\n",
      "Epoch 18, loss: 2.288248\n",
      "Epoch 19, loss: 2.289970\n",
      "Epoch 20, loss: 2.286916\n",
      "Epoch 21, loss: 2.282062\n",
      "Epoch 22, loss: 2.288213\n",
      "Epoch 23, loss: 2.283944\n",
      "Epoch 24, loss: 2.282007\n",
      "Epoch 25, loss: 2.282668\n",
      "Epoch 26, loss: 2.285681\n",
      "Epoch 27, loss: 2.280134\n",
      "Epoch 28, loss: 2.274991\n",
      "Epoch 29, loss: 2.284316\n",
      "Epoch 30, loss: 2.283565\n",
      "Epoch 31, loss: 2.280343\n",
      "Epoch 32, loss: 2.276519\n",
      "Epoch 33, loss: 2.269418\n",
      "Epoch 34, loss: 2.278952\n",
      "Epoch 35, loss: 2.267255\n",
      "Epoch 36, loss: 2.281414\n",
      "Epoch 37, loss: 2.279808\n",
      "Epoch 38, loss: 2.271864\n",
      "Epoch 39, loss: 2.260517\n",
      "Epoch 40, loss: 2.272668\n",
      "Epoch 41, loss: 2.257931\n",
      "Epoch 42, loss: 2.266002\n",
      "Epoch 43, loss: 2.271929\n",
      "Epoch 44, loss: 2.270032\n",
      "Epoch 45, loss: 2.263394\n",
      "Epoch 46, loss: 2.269240\n",
      "Epoch 47, loss: 2.265340\n",
      "Epoch 48, loss: 2.260231\n",
      "Epoch 49, loss: 2.262129\n",
      "Epoch 50, loss: 2.262984\n",
      "Epoch 51, loss: 2.275993\n",
      "Epoch 52, loss: 2.269465\n",
      "Epoch 53, loss: 2.259814\n",
      "Epoch 54, loss: 2.271774\n",
      "Epoch 55, loss: 2.253557\n",
      "Epoch 56, loss: 2.254499\n",
      "Epoch 57, loss: 2.248709\n",
      "Epoch 58, loss: 2.261241\n",
      "Epoch 59, loss: 2.262078\n",
      "Epoch 60, loss: 2.253159\n",
      "Epoch 61, loss: 2.264848\n",
      "Epoch 62, loss: 2.260755\n",
      "Epoch 63, loss: 2.268576\n",
      "Epoch 64, loss: 2.251592\n",
      "Epoch 65, loss: 2.251771\n",
      "Epoch 66, loss: 2.245629\n",
      "Epoch 67, loss: 2.253106\n",
      "Epoch 68, loss: 2.270450\n",
      "Epoch 69, loss: 2.260608\n",
      "Epoch 70, loss: 2.245258\n",
      "Epoch 71, loss: 2.253543\n",
      "Epoch 72, loss: 2.243908\n",
      "Epoch 73, loss: 2.241223\n",
      "Epoch 74, loss: 2.240074\n",
      "Epoch 75, loss: 2.237635\n",
      "Epoch 76, loss: 2.243428\n",
      "Epoch 77, loss: 2.259573\n",
      "Epoch 78, loss: 2.239395\n",
      "Epoch 79, loss: 2.251132\n",
      "Epoch 80, loss: 2.250179\n",
      "Epoch 81, loss: 2.253219\n",
      "Epoch 82, loss: 2.214922\n",
      "Epoch 83, loss: 2.242667\n",
      "Epoch 84, loss: 2.234578\n",
      "Epoch 85, loss: 2.243967\n",
      "Epoch 86, loss: 2.240738\n",
      "Epoch 87, loss: 2.240042\n",
      "Epoch 88, loss: 2.233351\n",
      "Epoch 89, loss: 2.245621\n",
      "Epoch 90, loss: 2.240797\n",
      "Epoch 91, loss: 2.247716\n",
      "Epoch 92, loss: 2.237017\n",
      "Epoch 93, loss: 2.247590\n",
      "Epoch 94, loss: 2.247252\n",
      "Epoch 95, loss: 2.236533\n",
      "Epoch 96, loss: 2.245389\n",
      "Epoch 97, loss: 2.232457\n",
      "Epoch 98, loss: 2.231331\n",
      "Epoch 99, loss: 2.237922\n",
      "Epoch 100, loss: 2.234932\n",
      "Epoch 101, loss: 2.223070\n",
      "Epoch 102, loss: 2.228076\n",
      "Epoch 103, loss: 2.257122\n",
      "Epoch 104, loss: 2.220834\n",
      "Epoch 105, loss: 2.244341\n",
      "Epoch 106, loss: 2.229182\n",
      "Epoch 107, loss: 2.223927\n",
      "Epoch 108, loss: 2.241388\n",
      "Epoch 109, loss: 2.216292\n",
      "Epoch 110, loss: 2.246020\n",
      "Epoch 111, loss: 2.224696\n",
      "Epoch 112, loss: 2.221173\n",
      "Epoch 113, loss: 2.253338\n",
      "Epoch 114, loss: 2.240125\n",
      "Epoch 115, loss: 2.238050\n",
      "Epoch 116, loss: 2.218672\n",
      "Epoch 117, loss: 2.229556\n",
      "Epoch 118, loss: 2.226640\n",
      "Epoch 119, loss: 2.223599\n",
      "Epoch 120, loss: 2.213999\n",
      "Epoch 121, loss: 2.224744\n",
      "Epoch 122, loss: 2.221662\n",
      "Epoch 123, loss: 2.234774\n",
      "Epoch 124, loss: 2.233540\n",
      "Epoch 125, loss: 2.240167\n",
      "Epoch 126, loss: 2.227995\n",
      "Epoch 127, loss: 2.238974\n",
      "Epoch 128, loss: 2.242122\n",
      "Epoch 129, loss: 2.231808\n",
      "Epoch 130, loss: 2.220907\n",
      "Epoch 131, loss: 2.231312\n",
      "Epoch 132, loss: 2.215348\n",
      "Epoch 133, loss: 2.227965\n",
      "Epoch 134, loss: 2.238738\n",
      "Epoch 135, loss: 2.216916\n",
      "Epoch 136, loss: 2.212235\n",
      "Epoch 137, loss: 2.205940\n",
      "Epoch 138, loss: 2.222306\n",
      "Epoch 139, loss: 2.210417\n",
      "Epoch 140, loss: 2.217950\n",
      "Epoch 141, loss: 2.238068\n",
      "Epoch 142, loss: 2.233624\n",
      "Epoch 143, loss: 2.218982\n",
      "Epoch 144, loss: 2.231476\n",
      "Epoch 145, loss: 2.216438\n",
      "Epoch 146, loss: 2.210226\n",
      "Epoch 147, loss: 2.220478\n",
      "Epoch 148, loss: 2.198169\n",
      "Epoch 149, loss: 2.215683\n",
      "Epoch 150, loss: 2.197523\n",
      "Epoch 151, loss: 2.217372\n",
      "Epoch 152, loss: 2.235739\n",
      "Epoch 153, loss: 2.205500\n",
      "Epoch 154, loss: 2.201162\n",
      "Epoch 155, loss: 2.219856\n",
      "Epoch 156, loss: 2.198721\n",
      "Epoch 157, loss: 2.224611\n",
      "Epoch 158, loss: 2.216742\n",
      "Epoch 159, loss: 2.250588\n",
      "Epoch 160, loss: 2.216080\n",
      "Epoch 161, loss: 2.235103\n",
      "Epoch 162, loss: 2.223468\n",
      "Epoch 163, loss: 2.220268\n",
      "Epoch 164, loss: 2.206254\n",
      "Epoch 165, loss: 2.207469\n",
      "Epoch 166, loss: 2.220028\n",
      "Epoch 167, loss: 2.241274\n",
      "Epoch 168, loss: 2.193074\n",
      "Epoch 169, loss: 2.206011\n",
      "Epoch 170, loss: 2.231314\n",
      "Epoch 171, loss: 2.191519\n",
      "Epoch 172, loss: 2.210710\n",
      "Epoch 173, loss: 2.224927\n",
      "Epoch 174, loss: 2.211280\n",
      "Epoch 175, loss: 2.220408\n",
      "Epoch 176, loss: 2.221517\n",
      "Epoch 177, loss: 2.223809\n",
      "Epoch 178, loss: 2.207381\n",
      "Epoch 179, loss: 2.216383\n",
      "Epoch 180, loss: 2.227772\n",
      "Epoch 181, loss: 2.205691\n",
      "Epoch 182, loss: 2.200501\n",
      "Epoch 183, loss: 2.240748\n",
      "Epoch 184, loss: 2.238082\n",
      "Epoch 185, loss: 2.171843\n",
      "Epoch 186, loss: 2.230017\n",
      "Epoch 187, loss: 2.195256\n",
      "Epoch 188, loss: 2.196645\n",
      "Epoch 189, loss: 2.186934\n",
      "Epoch 190, loss: 2.190865\n",
      "Epoch 191, loss: 2.208356\n",
      "Epoch 192, loss: 2.216433\n",
      "Epoch 193, loss: 2.202174\n",
      "Epoch 194, loss: 2.205026\n",
      "Epoch 195, loss: 2.213700\n",
      "Epoch 196, loss: 2.177329\n",
      "Epoch 197, loss: 2.216155\n",
      "Epoch 198, loss: 2.185509\n",
      "Epoch 199, loss: 2.204200\n",
      "==========\n",
      "learning rate = 0.001 reg = 1e-06 accuracy = 0.226 \n",
      "==========\n",
      "\n",
      "============Loop # 2...============\n",
      "Epoch 0, loss: 2.303361\n",
      "Epoch 1, loss: 2.301885\n",
      "Epoch 2, loss: 2.302058\n",
      "Epoch 3, loss: 2.301456\n",
      "Epoch 4, loss: 2.301939\n",
      "Epoch 5, loss: 2.301218\n",
      "Epoch 6, loss: 2.301730\n",
      "Epoch 7, loss: 2.302710\n",
      "Epoch 8, loss: 2.302184\n",
      "Epoch 9, loss: 2.301607\n",
      "Epoch 10, loss: 2.302244\n",
      "Epoch 11, loss: 2.301165\n",
      "Epoch 12, loss: 2.301816\n",
      "Epoch 13, loss: 2.302711\n",
      "Epoch 14, loss: 2.300680\n",
      "Epoch 15, loss: 2.301175\n",
      "Epoch 16, loss: 2.300165\n",
      "Epoch 17, loss: 2.299715\n",
      "Epoch 18, loss: 2.300322\n",
      "Epoch 19, loss: 2.301451\n",
      "Epoch 20, loss: 2.300808\n",
      "Epoch 21, loss: 2.300537\n",
      "Epoch 22, loss: 2.299878\n",
      "Epoch 23, loss: 2.300449\n",
      "Epoch 24, loss: 2.300935\n",
      "Epoch 25, loss: 2.300217\n",
      "Epoch 26, loss: 2.302431\n",
      "Epoch 27, loss: 2.299194\n",
      "Epoch 28, loss: 2.297993\n",
      "Epoch 29, loss: 2.299356\n",
      "Epoch 30, loss: 2.300672\n",
      "Epoch 31, loss: 2.300359\n",
      "Epoch 32, loss: 2.299715\n",
      "Epoch 33, loss: 2.298527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, loss: 2.299642\n",
      "Epoch 35, loss: 2.298979\n",
      "Epoch 36, loss: 2.299618\n",
      "Epoch 37, loss: 2.298298\n",
      "Epoch 38, loss: 2.298790\n",
      "Epoch 39, loss: 2.298817\n",
      "Epoch 40, loss: 2.299732\n",
      "Epoch 41, loss: 2.299211\n",
      "Epoch 42, loss: 2.298659\n",
      "Epoch 43, loss: 2.298114\n",
      "Epoch 44, loss: 2.299588\n",
      "Epoch 45, loss: 2.295744\n",
      "Epoch 46, loss: 2.298044\n",
      "Epoch 47, loss: 2.295900\n",
      "Epoch 48, loss: 2.298831\n",
      "Epoch 49, loss: 2.296846\n",
      "Epoch 50, loss: 2.296635\n",
      "Epoch 51, loss: 2.299225\n",
      "Epoch 52, loss: 2.296807\n",
      "Epoch 53, loss: 2.296665\n",
      "Epoch 54, loss: 2.297031\n",
      "Epoch 55, loss: 2.297750\n",
      "Epoch 56, loss: 2.296533\n",
      "Epoch 57, loss: 2.296706\n",
      "Epoch 58, loss: 2.294050\n",
      "Epoch 59, loss: 2.296184\n",
      "Epoch 60, loss: 2.296576\n",
      "Epoch 61, loss: 2.295749\n",
      "Epoch 62, loss: 2.298030\n",
      "Epoch 63, loss: 2.297075\n",
      "Epoch 64, loss: 2.294690\n",
      "Epoch 65, loss: 2.297250\n",
      "Epoch 66, loss: 2.296620\n",
      "Epoch 67, loss: 2.298423\n",
      "Epoch 68, loss: 2.296490\n",
      "Epoch 69, loss: 2.298523\n",
      "Epoch 70, loss: 2.297554\n",
      "Epoch 71, loss: 2.295075\n",
      "Epoch 72, loss: 2.298340\n",
      "Epoch 73, loss: 2.295497\n",
      "Epoch 74, loss: 2.294300\n",
      "Epoch 75, loss: 2.295425\n",
      "Epoch 76, loss: 2.297717\n",
      "Epoch 77, loss: 2.294628\n",
      "Epoch 78, loss: 2.295093\n",
      "Epoch 79, loss: 2.299639\n",
      "Epoch 80, loss: 2.292927\n",
      "Epoch 81, loss: 2.296695\n",
      "Epoch 82, loss: 2.293822\n",
      "Epoch 83, loss: 2.292123\n",
      "Epoch 84, loss: 2.293332\n",
      "Epoch 85, loss: 2.294899\n",
      "Epoch 86, loss: 2.296082\n",
      "Epoch 87, loss: 2.295167\n",
      "Epoch 88, loss: 2.296097\n",
      "Epoch 89, loss: 2.294720\n",
      "Epoch 90, loss: 2.296383\n",
      "Epoch 91, loss: 2.293269\n",
      "Epoch 92, loss: 2.292658\n",
      "Epoch 93, loss: 2.294940\n",
      "Epoch 94, loss: 2.297050\n",
      "Epoch 95, loss: 2.291833\n",
      "Epoch 96, loss: 2.294333\n",
      "Epoch 97, loss: 2.295194\n",
      "Epoch 98, loss: 2.294701\n",
      "Epoch 99, loss: 2.295760\n",
      "Epoch 100, loss: 2.294389\n",
      "Epoch 101, loss: 2.296196\n",
      "Epoch 102, loss: 2.291408\n",
      "Epoch 103, loss: 2.290327\n",
      "Epoch 104, loss: 2.294199\n",
      "Epoch 105, loss: 2.293114\n",
      "Epoch 106, loss: 2.294388\n",
      "Epoch 107, loss: 2.293797\n",
      "Epoch 108, loss: 2.292608\n",
      "Epoch 109, loss: 2.291611\n",
      "Epoch 110, loss: 2.293620\n",
      "Epoch 111, loss: 2.288447\n",
      "Epoch 112, loss: 2.292562\n",
      "Epoch 113, loss: 2.290636\n",
      "Epoch 114, loss: 2.292529\n",
      "Epoch 115, loss: 2.296587\n",
      "Epoch 116, loss: 2.293178\n",
      "Epoch 117, loss: 2.292131\n",
      "Epoch 118, loss: 2.291014\n",
      "Epoch 119, loss: 2.291237\n",
      "Epoch 120, loss: 2.290016\n",
      "Epoch 121, loss: 2.295669\n",
      "Epoch 122, loss: 2.293135\n",
      "Epoch 123, loss: 2.288039\n",
      "Epoch 124, loss: 2.288857\n",
      "Epoch 125, loss: 2.288768\n",
      "Epoch 126, loss: 2.293255\n",
      "Epoch 127, loss: 2.293821\n",
      "Epoch 128, loss: 2.286668\n",
      "Epoch 129, loss: 2.292662\n",
      "Epoch 130, loss: 2.291961\n",
      "Epoch 131, loss: 2.289580\n",
      "Epoch 132, loss: 2.293831\n",
      "Epoch 133, loss: 2.292194\n",
      "Epoch 134, loss: 2.288821\n",
      "Epoch 135, loss: 2.288690\n",
      "Epoch 136, loss: 2.294386\n",
      "Epoch 137, loss: 2.290273\n",
      "Epoch 138, loss: 2.289865\n",
      "Epoch 139, loss: 2.287778\n",
      "Epoch 140, loss: 2.289668\n",
      "Epoch 141, loss: 2.289822\n",
      "Epoch 142, loss: 2.289507\n",
      "Epoch 143, loss: 2.289471\n",
      "Epoch 144, loss: 2.291113\n",
      "Epoch 145, loss: 2.293644\n",
      "Epoch 146, loss: 2.289461\n",
      "Epoch 147, loss: 2.288697\n",
      "Epoch 148, loss: 2.288712\n",
      "Epoch 149, loss: 2.286822\n",
      "Epoch 150, loss: 2.289719\n",
      "Epoch 151, loss: 2.288462\n",
      "Epoch 152, loss: 2.287786\n",
      "Epoch 153, loss: 2.286583\n",
      "Epoch 154, loss: 2.291432\n",
      "Epoch 155, loss: 2.291295\n",
      "Epoch 156, loss: 2.293134\n",
      "Epoch 157, loss: 2.290741\n",
      "Epoch 158, loss: 2.290195\n",
      "Epoch 159, loss: 2.291195\n",
      "Epoch 160, loss: 2.288806\n",
      "Epoch 161, loss: 2.286330\n",
      "Epoch 162, loss: 2.288806\n",
      "Epoch 163, loss: 2.289037\n",
      "Epoch 164, loss: 2.286994\n",
      "Epoch 165, loss: 2.285441\n",
      "Epoch 166, loss: 2.286382\n",
      "Epoch 167, loss: 2.283541\n",
      "Epoch 168, loss: 2.287012\n",
      "Epoch 169, loss: 2.288536\n",
      "Epoch 170, loss: 2.286443\n",
      "Epoch 171, loss: 2.288463\n",
      "Epoch 172, loss: 2.282519\n",
      "Epoch 173, loss: 2.289934\n",
      "Epoch 174, loss: 2.289485\n",
      "Epoch 175, loss: 2.286836\n",
      "Epoch 176, loss: 2.283566\n",
      "Epoch 177, loss: 2.285734\n",
      "Epoch 178, loss: 2.277137\n",
      "Epoch 179, loss: 2.291309\n",
      "Epoch 180, loss: 2.288306\n",
      "Epoch 181, loss: 2.286603\n",
      "Epoch 182, loss: 2.282856\n",
      "Epoch 183, loss: 2.284514\n",
      "Epoch 184, loss: 2.290429\n",
      "Epoch 185, loss: 2.283152\n",
      "Epoch 186, loss: 2.287801\n",
      "Epoch 187, loss: 2.292626\n",
      "Epoch 188, loss: 2.290689\n",
      "Epoch 189, loss: 2.286343\n",
      "Epoch 190, loss: 2.288843\n",
      "Epoch 191, loss: 2.288660\n",
      "Epoch 192, loss: 2.279301\n",
      "Epoch 193, loss: 2.287267\n",
      "Epoch 194, loss: 2.285448\n",
      "Epoch 195, loss: 2.289366\n",
      "Epoch 196, loss: 2.282663\n",
      "Epoch 197, loss: 2.286682\n",
      "Epoch 198, loss: 2.286157\n",
      "Epoch 199, loss: 2.285122\n",
      "==========\n",
      "learning rate = 0.0001 reg = 0.0001 accuracy = 0.173 \n",
      "==========\n",
      "Epoch 0, loss: 2.302802\n",
      "Epoch 1, loss: 2.302145\n",
      "Epoch 2, loss: 2.303368\n",
      "Epoch 3, loss: 2.303323\n",
      "Epoch 4, loss: 2.301993\n",
      "Epoch 5, loss: 2.301882\n",
      "Epoch 6, loss: 2.301895\n",
      "Epoch 7, loss: 2.301284\n",
      "Epoch 8, loss: 2.300682\n",
      "Epoch 9, loss: 2.301841\n",
      "Epoch 10, loss: 2.301047\n",
      "Epoch 11, loss: 2.302012\n",
      "Epoch 12, loss: 2.300672\n",
      "Epoch 13, loss: 2.302188\n",
      "Epoch 14, loss: 2.301869\n",
      "Epoch 15, loss: 2.300059\n",
      "Epoch 16, loss: 2.301164\n",
      "Epoch 17, loss: 2.299553\n",
      "Epoch 18, loss: 2.301822\n",
      "Epoch 19, loss: 2.299783\n",
      "Epoch 20, loss: 2.299375\n",
      "Epoch 21, loss: 2.299449\n",
      "Epoch 22, loss: 2.300820\n",
      "Epoch 23, loss: 2.301917\n",
      "Epoch 24, loss: 2.300017\n",
      "Epoch 25, loss: 2.299609\n",
      "Epoch 26, loss: 2.300224\n",
      "Epoch 27, loss: 2.299554\n",
      "Epoch 28, loss: 2.301682\n",
      "Epoch 29, loss: 2.299316\n",
      "Epoch 30, loss: 2.299626\n",
      "Epoch 31, loss: 2.298609\n",
      "Epoch 32, loss: 2.298146\n",
      "Epoch 33, loss: 2.300969\n",
      "Epoch 34, loss: 2.298165\n",
      "Epoch 35, loss: 2.298984\n",
      "Epoch 36, loss: 2.298194\n",
      "Epoch 37, loss: 2.298743\n",
      "Epoch 38, loss: 2.299627\n",
      "Epoch 39, loss: 2.298495\n",
      "Epoch 40, loss: 2.298165\n",
      "Epoch 41, loss: 2.297209\n",
      "Epoch 42, loss: 2.297461\n",
      "Epoch 43, loss: 2.297039\n",
      "Epoch 44, loss: 2.296998\n",
      "Epoch 45, loss: 2.296994\n",
      "Epoch 46, loss: 2.296654\n",
      "Epoch 47, loss: 2.297516\n",
      "Epoch 48, loss: 2.299454\n",
      "Epoch 49, loss: 2.299608\n",
      "Epoch 50, loss: 2.297988\n",
      "Epoch 51, loss: 2.296729\n",
      "Epoch 52, loss: 2.300097\n",
      "Epoch 53, loss: 2.297643\n",
      "Epoch 54, loss: 2.298064\n",
      "Epoch 55, loss: 2.297820\n",
      "Epoch 56, loss: 2.295971\n",
      "Epoch 57, loss: 2.297569\n",
      "Epoch 58, loss: 2.296403\n",
      "Epoch 59, loss: 2.297945\n",
      "Epoch 60, loss: 2.295898\n",
      "Epoch 61, loss: 2.298815\n",
      "Epoch 62, loss: 2.299905\n",
      "Epoch 63, loss: 2.299708\n",
      "Epoch 64, loss: 2.294822\n",
      "Epoch 65, loss: 2.296015\n",
      "Epoch 66, loss: 2.296677\n",
      "Epoch 67, loss: 2.295020\n",
      "Epoch 68, loss: 2.296390\n",
      "Epoch 69, loss: 2.295458\n",
      "Epoch 70, loss: 2.295029\n",
      "Epoch 71, loss: 2.295139\n",
      "Epoch 72, loss: 2.292683\n",
      "Epoch 73, loss: 2.295615\n",
      "Epoch 74, loss: 2.297490\n",
      "Epoch 75, loss: 2.295477\n",
      "Epoch 76, loss: 2.296761\n",
      "Epoch 77, loss: 2.296088\n",
      "Epoch 78, loss: 2.296514\n",
      "Epoch 79, loss: 2.296723\n",
      "Epoch 80, loss: 2.293430\n",
      "Epoch 81, loss: 2.292732\n",
      "Epoch 82, loss: 2.290546\n",
      "Epoch 83, loss: 2.295345\n",
      "Epoch 84, loss: 2.294921\n",
      "Epoch 85, loss: 2.293705\n",
      "Epoch 86, loss: 2.293464\n",
      "Epoch 87, loss: 2.293557\n",
      "Epoch 88, loss: 2.297053\n",
      "Epoch 89, loss: 2.293175\n",
      "Epoch 90, loss: 2.291313\n",
      "Epoch 91, loss: 2.295829\n",
      "Epoch 92, loss: 2.296473\n",
      "Epoch 93, loss: 2.290385\n",
      "Epoch 94, loss: 2.294178\n",
      "Epoch 95, loss: 2.295107\n",
      "Epoch 96, loss: 2.294233\n",
      "Epoch 97, loss: 2.296641\n",
      "Epoch 98, loss: 2.293931\n",
      "Epoch 99, loss: 2.290291\n",
      "Epoch 100, loss: 2.291243\n",
      "Epoch 101, loss: 2.296554\n",
      "Epoch 102, loss: 2.294483\n",
      "Epoch 103, loss: 2.291817\n",
      "Epoch 104, loss: 2.291506\n",
      "Epoch 105, loss: 2.289682\n",
      "Epoch 106, loss: 2.294212\n",
      "Epoch 107, loss: 2.291839\n",
      "Epoch 108, loss: 2.293506\n",
      "Epoch 109, loss: 2.290683\n",
      "Epoch 110, loss: 2.290749\n",
      "Epoch 111, loss: 2.293495\n",
      "Epoch 112, loss: 2.292235\n",
      "Epoch 113, loss: 2.291688\n",
      "Epoch 114, loss: 2.291095\n",
      "Epoch 115, loss: 2.290898\n",
      "Epoch 116, loss: 2.288979\n",
      "Epoch 117, loss: 2.289565\n",
      "Epoch 118, loss: 2.292168\n",
      "Epoch 119, loss: 2.292996\n",
      "Epoch 120, loss: 2.294150\n",
      "Epoch 121, loss: 2.291452\n",
      "Epoch 122, loss: 2.292527\n",
      "Epoch 123, loss: 2.294817\n",
      "Epoch 124, loss: 2.290021\n",
      "Epoch 125, loss: 2.292573\n",
      "Epoch 126, loss: 2.288127\n",
      "Epoch 127, loss: 2.291192\n",
      "Epoch 128, loss: 2.293117\n",
      "Epoch 129, loss: 2.292974\n",
      "Epoch 130, loss: 2.289157\n",
      "Epoch 131, loss: 2.289410\n",
      "Epoch 132, loss: 2.287050\n",
      "Epoch 133, loss: 2.290369\n",
      "Epoch 134, loss: 2.290307\n",
      "Epoch 135, loss: 2.290550\n",
      "Epoch 136, loss: 2.292126\n",
      "Epoch 137, loss: 2.290075\n",
      "Epoch 138, loss: 2.288485\n",
      "Epoch 139, loss: 2.291866\n",
      "Epoch 140, loss: 2.290170\n",
      "Epoch 141, loss: 2.287642\n",
      "Epoch 142, loss: 2.293567\n",
      "Epoch 143, loss: 2.285115\n",
      "Epoch 144, loss: 2.294030\n",
      "Epoch 145, loss: 2.292077\n",
      "Epoch 146, loss: 2.288438\n",
      "Epoch 147, loss: 2.285813\n",
      "Epoch 148, loss: 2.290285\n",
      "Epoch 149, loss: 2.286468\n",
      "Epoch 150, loss: 2.288404\n",
      "Epoch 151, loss: 2.286018\n",
      "Epoch 152, loss: 2.288317\n",
      "Epoch 153, loss: 2.288520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154, loss: 2.292174\n",
      "Epoch 155, loss: 2.284544\n",
      "Epoch 156, loss: 2.290166\n",
      "Epoch 157, loss: 2.285529\n",
      "Epoch 158, loss: 2.290364\n",
      "Epoch 159, loss: 2.286442\n",
      "Epoch 160, loss: 2.289013\n",
      "Epoch 161, loss: 2.286917\n",
      "Epoch 162, loss: 2.290442\n",
      "Epoch 163, loss: 2.290733\n",
      "Epoch 164, loss: 2.287340\n",
      "Epoch 165, loss: 2.282996\n",
      "Epoch 166, loss: 2.290337\n",
      "Epoch 167, loss: 2.288209\n",
      "Epoch 168, loss: 2.288663\n",
      "Epoch 169, loss: 2.284933\n",
      "Epoch 170, loss: 2.283661\n",
      "Epoch 171, loss: 2.292832\n",
      "Epoch 172, loss: 2.288645\n",
      "Epoch 173, loss: 2.289705\n",
      "Epoch 174, loss: 2.287268\n",
      "Epoch 175, loss: 2.289251\n",
      "Epoch 176, loss: 2.288592\n",
      "Epoch 177, loss: 2.291792\n",
      "Epoch 178, loss: 2.284443\n",
      "Epoch 179, loss: 2.287690\n",
      "Epoch 180, loss: 2.284324\n",
      "Epoch 181, loss: 2.285855\n",
      "Epoch 182, loss: 2.287009\n",
      "Epoch 183, loss: 2.281882\n",
      "Epoch 184, loss: 2.285425\n",
      "Epoch 185, loss: 2.280484\n",
      "Epoch 186, loss: 2.287077\n",
      "Epoch 187, loss: 2.286448\n",
      "Epoch 188, loss: 2.285336\n",
      "Epoch 189, loss: 2.285381\n",
      "Epoch 190, loss: 2.284928\n",
      "Epoch 191, loss: 2.284608\n",
      "Epoch 192, loss: 2.287104\n",
      "Epoch 193, loss: 2.288171\n",
      "Epoch 194, loss: 2.283147\n",
      "Epoch 195, loss: 2.288150\n",
      "Epoch 196, loss: 2.285910\n",
      "Epoch 197, loss: 2.283749\n",
      "Epoch 198, loss: 2.285606\n",
      "Epoch 199, loss: 2.292229\n",
      "==========\n",
      "learning rate = 0.0001 reg = 1e-05 accuracy = 0.173 \n",
      "==========\n",
      "Epoch 0, loss: 2.303003\n",
      "Epoch 1, loss: 2.302083\n",
      "Epoch 2, loss: 2.303465\n",
      "Epoch 3, loss: 2.301429\n",
      "Epoch 4, loss: 2.302089\n",
      "Epoch 5, loss: 2.301647\n",
      "Epoch 6, loss: 2.301500\n",
      "Epoch 7, loss: 2.301731\n",
      "Epoch 8, loss: 2.301368\n",
      "Epoch 9, loss: 2.301323\n",
      "Epoch 10, loss: 2.302062\n",
      "Epoch 11, loss: 2.302019\n",
      "Epoch 12, loss: 2.301520\n",
      "Epoch 13, loss: 2.302020\n",
      "Epoch 14, loss: 2.300598\n",
      "Epoch 15, loss: 2.300577\n",
      "Epoch 16, loss: 2.301299\n",
      "Epoch 17, loss: 2.300975\n",
      "Epoch 18, loss: 2.299869\n",
      "Epoch 19, loss: 2.300794\n",
      "Epoch 20, loss: 2.300316\n",
      "Epoch 21, loss: 2.299383\n",
      "Epoch 22, loss: 2.300256\n",
      "Epoch 23, loss: 2.300603\n",
      "Epoch 24, loss: 2.298624\n",
      "Epoch 25, loss: 2.300194\n",
      "Epoch 26, loss: 2.299463\n",
      "Epoch 27, loss: 2.299989\n",
      "Epoch 28, loss: 2.300703\n",
      "Epoch 29, loss: 2.298385\n",
      "Epoch 30, loss: 2.299836\n",
      "Epoch 31, loss: 2.298898\n",
      "Epoch 32, loss: 2.298498\n",
      "Epoch 33, loss: 2.298252\n",
      "Epoch 34, loss: 2.298784\n",
      "Epoch 35, loss: 2.298699\n",
      "Epoch 36, loss: 2.299409\n",
      "Epoch 37, loss: 2.298790\n",
      "Epoch 38, loss: 2.298199\n",
      "Epoch 39, loss: 2.300583\n",
      "Epoch 40, loss: 2.295174\n",
      "Epoch 41, loss: 2.299536\n",
      "Epoch 42, loss: 2.299786\n",
      "Epoch 43, loss: 2.300421\n",
      "Epoch 44, loss: 2.300867\n",
      "Epoch 45, loss: 2.298410\n",
      "Epoch 46, loss: 2.296575\n",
      "Epoch 47, loss: 2.301371\n",
      "Epoch 48, loss: 2.297208\n",
      "Epoch 49, loss: 2.295094\n",
      "Epoch 50, loss: 2.298179\n",
      "Epoch 51, loss: 2.300650\n",
      "Epoch 52, loss: 2.296020\n",
      "Epoch 53, loss: 2.296907\n",
      "Epoch 54, loss: 2.299200\n",
      "Epoch 55, loss: 2.297169\n",
      "Epoch 56, loss: 2.296841\n",
      "Epoch 57, loss: 2.295793\n",
      "Epoch 58, loss: 2.298203\n",
      "Epoch 59, loss: 2.297837\n",
      "Epoch 60, loss: 2.295892\n",
      "Epoch 61, loss: 2.296199\n",
      "Epoch 62, loss: 2.296581\n",
      "Epoch 63, loss: 2.297519\n",
      "Epoch 64, loss: 2.295924\n",
      "Epoch 65, loss: 2.296157\n",
      "Epoch 66, loss: 2.294964\n",
      "Epoch 67, loss: 2.293979\n",
      "Epoch 68, loss: 2.293944\n",
      "Epoch 69, loss: 2.295530\n",
      "Epoch 70, loss: 2.293969\n",
      "Epoch 71, loss: 2.293296\n",
      "Epoch 72, loss: 2.297201\n",
      "Epoch 73, loss: 2.292750\n",
      "Epoch 74, loss: 2.296961\n",
      "Epoch 75, loss: 2.294865\n",
      "Epoch 76, loss: 2.297471\n",
      "Epoch 77, loss: 2.295799\n",
      "Epoch 78, loss: 2.296937\n",
      "Epoch 79, loss: 2.296222\n",
      "Epoch 80, loss: 2.296563\n",
      "Epoch 81, loss: 2.294972\n",
      "Epoch 82, loss: 2.295602\n",
      "Epoch 83, loss: 2.294579\n",
      "Epoch 84, loss: 2.294612\n",
      "Epoch 85, loss: 2.292992\n",
      "Epoch 86, loss: 2.293912\n",
      "Epoch 87, loss: 2.295189\n",
      "Epoch 88, loss: 2.293068\n",
      "Epoch 89, loss: 2.292496\n",
      "Epoch 90, loss: 2.290271\n",
      "Epoch 91, loss: 2.294438\n",
      "Epoch 92, loss: 2.294617\n",
      "Epoch 93, loss: 2.296143\n",
      "Epoch 94, loss: 2.292456\n",
      "Epoch 95, loss: 2.293691\n",
      "Epoch 96, loss: 2.291466\n",
      "Epoch 97, loss: 2.296274\n",
      "Epoch 98, loss: 2.291660\n",
      "Epoch 99, loss: 2.294021\n",
      "Epoch 100, loss: 2.290433\n",
      "Epoch 101, loss: 2.297203\n",
      "Epoch 102, loss: 2.291598\n",
      "Epoch 103, loss: 2.292977\n",
      "Epoch 104, loss: 2.290797\n",
      "Epoch 105, loss: 2.291154\n",
      "Epoch 106, loss: 2.297588\n",
      "Epoch 107, loss: 2.290548\n",
      "Epoch 108, loss: 2.292036\n",
      "Epoch 109, loss: 2.289504\n",
      "Epoch 110, loss: 2.292219\n",
      "Epoch 111, loss: 2.295449\n",
      "Epoch 112, loss: 2.291461\n",
      "Epoch 113, loss: 2.292713\n",
      "Epoch 114, loss: 2.294424\n",
      "Epoch 115, loss: 2.293321\n",
      "Epoch 116, loss: 2.291299\n",
      "Epoch 117, loss: 2.288460\n",
      "Epoch 118, loss: 2.293070\n",
      "Epoch 119, loss: 2.292663\n",
      "Epoch 120, loss: 2.293707\n",
      "Epoch 121, loss: 2.292607\n",
      "Epoch 122, loss: 2.293044\n",
      "Epoch 123, loss: 2.290689\n",
      "Epoch 124, loss: 2.291198\n",
      "Epoch 125, loss: 2.290581\n",
      "Epoch 126, loss: 2.293361\n",
      "Epoch 127, loss: 2.289806\n",
      "Epoch 128, loss: 2.286580\n",
      "Epoch 129, loss: 2.294873\n",
      "Epoch 130, loss: 2.289535\n",
      "Epoch 131, loss: 2.292928\n",
      "Epoch 132, loss: 2.293590\n",
      "Epoch 133, loss: 2.291347\n",
      "Epoch 134, loss: 2.290117\n",
      "Epoch 135, loss: 2.288880\n",
      "Epoch 136, loss: 2.289252\n",
      "Epoch 137, loss: 2.289352\n",
      "Epoch 138, loss: 2.293559\n",
      "Epoch 139, loss: 2.290699\n",
      "Epoch 140, loss: 2.293094\n",
      "Epoch 141, loss: 2.289169\n",
      "Epoch 142, loss: 2.293079\n",
      "Epoch 143, loss: 2.291947\n",
      "Epoch 144, loss: 2.293317\n",
      "Epoch 145, loss: 2.290737\n",
      "Epoch 146, loss: 2.293533\n",
      "Epoch 147, loss: 2.288437\n",
      "Epoch 148, loss: 2.288313\n",
      "Epoch 149, loss: 2.291773\n",
      "Epoch 150, loss: 2.284720\n",
      "Epoch 151, loss: 2.286398\n",
      "Epoch 152, loss: 2.288811\n",
      "Epoch 153, loss: 2.286480\n",
      "Epoch 154, loss: 2.292081\n",
      "Epoch 155, loss: 2.282879\n",
      "Epoch 156, loss: 2.285776\n",
      "Epoch 157, loss: 2.287028\n",
      "Epoch 158, loss: 2.286316\n",
      "Epoch 159, loss: 2.285591\n",
      "Epoch 160, loss: 2.289342\n",
      "Epoch 161, loss: 2.290685\n",
      "Epoch 162, loss: 2.288221\n",
      "Epoch 163, loss: 2.291438\n",
      "Epoch 164, loss: 2.290121\n",
      "Epoch 165, loss: 2.289373\n",
      "Epoch 166, loss: 2.288273\n",
      "Epoch 167, loss: 2.292197\n",
      "Epoch 168, loss: 2.286828\n",
      "Epoch 169, loss: 2.285915\n",
      "Epoch 170, loss: 2.289399\n",
      "Epoch 171, loss: 2.288208\n",
      "Epoch 172, loss: 2.289974\n",
      "Epoch 173, loss: 2.291230\n",
      "Epoch 174, loss: 2.289923\n",
      "Epoch 175, loss: 2.289992\n",
      "Epoch 176, loss: 2.287316\n",
      "Epoch 177, loss: 2.287953\n",
      "Epoch 178, loss: 2.290161\n",
      "Epoch 179, loss: 2.285748\n",
      "Epoch 180, loss: 2.285099\n",
      "Epoch 181, loss: 2.293499\n",
      "Epoch 182, loss: 2.282053\n",
      "Epoch 183, loss: 2.288547\n",
      "Epoch 184, loss: 2.288374\n",
      "Epoch 185, loss: 2.288566\n",
      "Epoch 186, loss: 2.288076\n",
      "Epoch 187, loss: 2.293250\n",
      "Epoch 188, loss: 2.286894\n",
      "Epoch 189, loss: 2.285673\n",
      "Epoch 190, loss: 2.289216\n",
      "Epoch 191, loss: 2.289167\n",
      "Epoch 192, loss: 2.289352\n",
      "Epoch 193, loss: 2.283543\n",
      "Epoch 194, loss: 2.281440\n",
      "Epoch 195, loss: 2.292766\n",
      "Epoch 196, loss: 2.286066\n",
      "Epoch 197, loss: 2.286161\n",
      "Epoch 198, loss: 2.283171\n",
      "Epoch 199, loss: 2.279016\n",
      "==========\n",
      "learning rate = 0.0001 reg = 1e-06 accuracy = 0.165 \n",
      "==========\n",
      "\n",
      "============Loop # 3...============\n",
      "Epoch 0, loss: 2.303578\n",
      "Epoch 1, loss: 2.302091\n",
      "Epoch 2, loss: 2.303272\n",
      "Epoch 3, loss: 2.303121\n",
      "Epoch 4, loss: 2.302157\n",
      "Epoch 5, loss: 2.303574\n",
      "Epoch 6, loss: 2.301854\n",
      "Epoch 7, loss: 2.303260\n",
      "Epoch 8, loss: 2.301148\n",
      "Epoch 9, loss: 2.302503\n",
      "Epoch 10, loss: 2.302409\n",
      "Epoch 11, loss: 2.301585\n",
      "Epoch 12, loss: 2.303333\n",
      "Epoch 13, loss: 2.303836\n",
      "Epoch 14, loss: 2.303107\n",
      "Epoch 15, loss: 2.301683\n",
      "Epoch 16, loss: 2.302146\n",
      "Epoch 17, loss: 2.303047\n",
      "Epoch 18, loss: 2.303858\n",
      "Epoch 19, loss: 2.303518\n",
      "Epoch 20, loss: 2.301081\n",
      "Epoch 21, loss: 2.302393\n",
      "Epoch 22, loss: 2.302864\n",
      "Epoch 23, loss: 2.302686\n",
      "Epoch 24, loss: 2.301073\n",
      "Epoch 25, loss: 2.301987\n",
      "Epoch 26, loss: 2.301556\n",
      "Epoch 27, loss: 2.303481\n",
      "Epoch 28, loss: 2.302157\n",
      "Epoch 29, loss: 2.302072\n",
      "Epoch 30, loss: 2.302003\n",
      "Epoch 31, loss: 2.301890\n",
      "Epoch 32, loss: 2.301741\n",
      "Epoch 33, loss: 2.303186\n",
      "Epoch 34, loss: 2.302367\n",
      "Epoch 35, loss: 2.301681\n",
      "Epoch 36, loss: 2.302137\n",
      "Epoch 37, loss: 2.301349\n",
      "Epoch 38, loss: 2.301330\n",
      "Epoch 39, loss: 2.303673\n",
      "Epoch 40, loss: 2.302648\n",
      "Epoch 41, loss: 2.302961\n",
      "Epoch 42, loss: 2.302330\n",
      "Epoch 43, loss: 2.302602\n",
      "Epoch 44, loss: 2.302657\n",
      "Epoch 45, loss: 2.302729\n",
      "Epoch 46, loss: 2.301436\n",
      "Epoch 47, loss: 2.302717\n",
      "Epoch 48, loss: 2.301899\n",
      "Epoch 49, loss: 2.301866\n",
      "Epoch 50, loss: 2.301275\n",
      "Epoch 51, loss: 2.302675\n",
      "Epoch 52, loss: 2.302346\n",
      "Epoch 53, loss: 2.301586\n",
      "Epoch 54, loss: 2.302288\n",
      "Epoch 55, loss: 2.302680\n",
      "Epoch 56, loss: 2.300358\n",
      "Epoch 57, loss: 2.302692\n",
      "Epoch 58, loss: 2.302470\n",
      "Epoch 59, loss: 2.301819\n",
      "Epoch 60, loss: 2.301515\n",
      "Epoch 61, loss: 2.300768\n",
      "Epoch 62, loss: 2.303271\n",
      "Epoch 63, loss: 2.301918\n",
      "Epoch 64, loss: 2.301994\n",
      "Epoch 65, loss: 2.301742\n",
      "Epoch 66, loss: 2.301596\n",
      "Epoch 67, loss: 2.302124\n",
      "Epoch 68, loss: 2.301177\n",
      "Epoch 69, loss: 2.302318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, loss: 2.302321\n",
      "Epoch 71, loss: 2.302586\n",
      "Epoch 72, loss: 2.301563\n",
      "Epoch 73, loss: 2.301636\n",
      "Epoch 74, loss: 2.300827\n",
      "Epoch 75, loss: 2.302413\n",
      "Epoch 76, loss: 2.301509\n",
      "Epoch 77, loss: 2.301891\n",
      "Epoch 78, loss: 2.302202\n",
      "Epoch 79, loss: 2.302784\n",
      "Epoch 80, loss: 2.302734\n",
      "Epoch 81, loss: 2.301849\n",
      "Epoch 82, loss: 2.302054\n",
      "Epoch 83, loss: 2.302484\n",
      "Epoch 84, loss: 2.301455\n",
      "Epoch 85, loss: 2.300682\n",
      "Epoch 86, loss: 2.302186\n",
      "Epoch 87, loss: 2.301712\n",
      "Epoch 88, loss: 2.301095\n",
      "Epoch 89, loss: 2.300423\n",
      "Epoch 90, loss: 2.301954\n",
      "Epoch 91, loss: 2.300433\n",
      "Epoch 92, loss: 2.302303\n",
      "Epoch 93, loss: 2.301080\n",
      "Epoch 94, loss: 2.301039\n",
      "Epoch 95, loss: 2.300779\n",
      "Epoch 96, loss: 2.301832\n",
      "Epoch 97, loss: 2.301499\n",
      "Epoch 98, loss: 2.302370\n",
      "Epoch 99, loss: 2.301295\n",
      "Epoch 100, loss: 2.301012\n",
      "Epoch 101, loss: 2.301955\n",
      "Epoch 102, loss: 2.301659\n",
      "Epoch 103, loss: 2.301586\n",
      "Epoch 104, loss: 2.301793\n",
      "Epoch 105, loss: 2.301996\n",
      "Epoch 106, loss: 2.300934\n",
      "Epoch 107, loss: 2.299939\n",
      "Epoch 108, loss: 2.301035\n",
      "Epoch 109, loss: 2.301640\n",
      "Epoch 110, loss: 2.300922\n",
      "Epoch 111, loss: 2.300642\n",
      "Epoch 112, loss: 2.300321\n",
      "Epoch 113, loss: 2.302065\n",
      "Epoch 114, loss: 2.301356\n",
      "Epoch 115, loss: 2.302165\n",
      "Epoch 116, loss: 2.301731\n",
      "Epoch 117, loss: 2.302048\n",
      "Epoch 118, loss: 2.300464\n",
      "Epoch 119, loss: 2.301743\n",
      "Epoch 120, loss: 2.300976\n",
      "Epoch 121, loss: 2.301943\n",
      "Epoch 122, loss: 2.300773\n",
      "Epoch 123, loss: 2.300826\n",
      "Epoch 124, loss: 2.301720\n",
      "Epoch 125, loss: 2.300697\n",
      "Epoch 126, loss: 2.299935\n",
      "Epoch 127, loss: 2.300910\n",
      "Epoch 128, loss: 2.301351\n",
      "Epoch 129, loss: 2.301331\n",
      "Epoch 130, loss: 2.299133\n",
      "Epoch 131, loss: 2.301527\n",
      "Epoch 132, loss: 2.300637\n",
      "Epoch 133, loss: 2.300428\n",
      "Epoch 134, loss: 2.300235\n",
      "Epoch 135, loss: 2.301371\n",
      "Epoch 136, loss: 2.302071\n",
      "Epoch 137, loss: 2.299333\n",
      "Epoch 138, loss: 2.301008\n",
      "Epoch 139, loss: 2.301795\n",
      "Epoch 140, loss: 2.301868\n",
      "Epoch 141, loss: 2.301825\n",
      "Epoch 142, loss: 2.301007\n",
      "Epoch 143, loss: 2.301521\n",
      "Epoch 144, loss: 2.302338\n",
      "Epoch 145, loss: 2.301305\n",
      "Epoch 146, loss: 2.301418\n",
      "Epoch 147, loss: 2.301953\n",
      "Epoch 148, loss: 2.302277\n",
      "Epoch 149, loss: 2.300289\n",
      "Epoch 150, loss: 2.300814\n",
      "Epoch 151, loss: 2.301624\n",
      "Epoch 152, loss: 2.301195\n",
      "Epoch 153, loss: 2.300195\n",
      "Epoch 154, loss: 2.301329\n",
      "Epoch 155, loss: 2.301386\n",
      "Epoch 156, loss: 2.301223\n",
      "Epoch 157, loss: 2.300534\n",
      "Epoch 158, loss: 2.302402\n",
      "Epoch 159, loss: 2.300249\n",
      "Epoch 160, loss: 2.300292\n",
      "Epoch 161, loss: 2.301751\n",
      "Epoch 162, loss: 2.301436\n",
      "Epoch 163, loss: 2.301925\n",
      "Epoch 164, loss: 2.300952\n",
      "Epoch 165, loss: 2.301743\n",
      "Epoch 166, loss: 2.300585\n",
      "Epoch 167, loss: 2.300688\n",
      "Epoch 168, loss: 2.301791\n",
      "Epoch 169, loss: 2.300154\n",
      "Epoch 170, loss: 2.299523\n",
      "Epoch 171, loss: 2.300785\n",
      "Epoch 172, loss: 2.300766\n",
      "Epoch 173, loss: 2.300076\n",
      "Epoch 174, loss: 2.301167\n",
      "Epoch 175, loss: 2.301522\n",
      "Epoch 176, loss: 2.300843\n",
      "Epoch 177, loss: 2.301294\n",
      "Epoch 178, loss: 2.300267\n",
      "Epoch 179, loss: 2.300597\n",
      "Epoch 180, loss: 2.300805\n",
      "Epoch 181, loss: 2.300989\n",
      "Epoch 182, loss: 2.298743\n",
      "Epoch 183, loss: 2.299814\n",
      "Epoch 184, loss: 2.299434\n",
      "Epoch 185, loss: 2.300545\n",
      "Epoch 186, loss: 2.300895\n",
      "Epoch 187, loss: 2.300470\n",
      "Epoch 188, loss: 2.300723\n",
      "Epoch 189, loss: 2.301814\n",
      "Epoch 190, loss: 2.299946\n",
      "Epoch 191, loss: 2.301432\n",
      "Epoch 192, loss: 2.301542\n",
      "Epoch 193, loss: 2.299886\n",
      "Epoch 194, loss: 2.300641\n",
      "Epoch 195, loss: 2.301050\n",
      "Epoch 196, loss: 2.300586\n",
      "Epoch 197, loss: 2.300493\n",
      "Epoch 198, loss: 2.299946\n",
      "Epoch 199, loss: 2.301847\n",
      "==========\n",
      "learning rate = 1e-05 reg = 0.0001 accuracy = 0.113 \n",
      "==========\n",
      "Epoch 0, loss: 2.303155\n",
      "Epoch 1, loss: 2.302605\n",
      "Epoch 2, loss: 2.303367\n",
      "Epoch 3, loss: 2.302618\n",
      "Epoch 4, loss: 2.303287\n",
      "Epoch 5, loss: 2.303358\n",
      "Epoch 6, loss: 2.302341\n",
      "Epoch 7, loss: 2.302484\n",
      "Epoch 8, loss: 2.302194\n",
      "Epoch 9, loss: 2.302651\n",
      "Epoch 10, loss: 2.301819\n",
      "Epoch 11, loss: 2.302358\n",
      "Epoch 12, loss: 2.302551\n",
      "Epoch 13, loss: 2.302644\n",
      "Epoch 14, loss: 2.301984\n",
      "Epoch 15, loss: 2.303225\n",
      "Epoch 16, loss: 2.302012\n",
      "Epoch 17, loss: 2.303247\n",
      "Epoch 18, loss: 2.301964\n",
      "Epoch 19, loss: 2.302885\n",
      "Epoch 20, loss: 2.302757\n",
      "Epoch 21, loss: 2.301979\n",
      "Epoch 22, loss: 2.302880\n",
      "Epoch 23, loss: 2.302595\n",
      "Epoch 24, loss: 2.302178\n",
      "Epoch 25, loss: 2.302585\n",
      "Epoch 26, loss: 2.302748\n",
      "Epoch 27, loss: 2.302870\n",
      "Epoch 28, loss: 2.302225\n",
      "Epoch 29, loss: 2.302909\n",
      "Epoch 30, loss: 2.302402\n",
      "Epoch 31, loss: 2.302817\n",
      "Epoch 32, loss: 2.303051\n",
      "Epoch 33, loss: 2.302578\n",
      "Epoch 34, loss: 2.301949\n",
      "Epoch 35, loss: 2.302763\n",
      "Epoch 36, loss: 2.302354\n",
      "Epoch 37, loss: 2.301505\n",
      "Epoch 38, loss: 2.302064\n",
      "Epoch 39, loss: 2.302580\n",
      "Epoch 40, loss: 2.302379\n",
      "Epoch 41, loss: 2.303320\n",
      "Epoch 42, loss: 2.302883\n",
      "Epoch 43, loss: 2.303642\n",
      "Epoch 44, loss: 2.303008\n",
      "Epoch 45, loss: 2.302175\n",
      "Epoch 46, loss: 2.302152\n",
      "Epoch 47, loss: 2.301771\n",
      "Epoch 48, loss: 2.302018\n",
      "Epoch 49, loss: 2.302397\n",
      "Epoch 50, loss: 2.301783\n",
      "Epoch 51, loss: 2.302064\n",
      "Epoch 52, loss: 2.301440\n",
      "Epoch 53, loss: 2.302409\n",
      "Epoch 54, loss: 2.302107\n",
      "Epoch 55, loss: 2.302479\n",
      "Epoch 56, loss: 2.301804\n",
      "Epoch 57, loss: 2.302226\n",
      "Epoch 58, loss: 2.302698\n",
      "Epoch 59, loss: 2.303179\n",
      "Epoch 60, loss: 2.302552\n",
      "Epoch 61, loss: 2.302608\n",
      "Epoch 62, loss: 2.301549\n",
      "Epoch 63, loss: 2.302758\n",
      "Epoch 64, loss: 2.301571\n",
      "Epoch 65, loss: 2.302839\n",
      "Epoch 66, loss: 2.301708\n",
      "Epoch 67, loss: 2.302766\n",
      "Epoch 68, loss: 2.301740\n",
      "Epoch 69, loss: 2.302085\n",
      "Epoch 70, loss: 2.301332\n",
      "Epoch 71, loss: 2.302919\n",
      "Epoch 72, loss: 2.303332\n",
      "Epoch 73, loss: 2.302304\n",
      "Epoch 74, loss: 2.301685\n",
      "Epoch 75, loss: 2.302637\n",
      "Epoch 76, loss: 2.301924\n",
      "Epoch 77, loss: 2.301597\n",
      "Epoch 78, loss: 2.302393\n",
      "Epoch 79, loss: 2.302101\n",
      "Epoch 80, loss: 2.301389\n",
      "Epoch 81, loss: 2.302640\n",
      "Epoch 82, loss: 2.302419\n",
      "Epoch 83, loss: 2.302494\n",
      "Epoch 84, loss: 2.301743\n",
      "Epoch 85, loss: 2.301539\n",
      "Epoch 86, loss: 2.301709\n",
      "Epoch 87, loss: 2.301095\n",
      "Epoch 88, loss: 2.302332\n",
      "Epoch 89, loss: 2.302080\n",
      "Epoch 90, loss: 2.301580\n",
      "Epoch 91, loss: 2.302362\n",
      "Epoch 92, loss: 2.301442\n",
      "Epoch 93, loss: 2.302508\n",
      "Epoch 94, loss: 2.302245\n",
      "Epoch 95, loss: 2.302120\n",
      "Epoch 96, loss: 2.302051\n",
      "Epoch 97, loss: 2.302216\n",
      "Epoch 98, loss: 2.303125\n",
      "Epoch 99, loss: 2.302847\n",
      "Epoch 100, loss: 2.301964\n",
      "Epoch 101, loss: 2.303572\n",
      "Epoch 102, loss: 2.301661\n",
      "Epoch 103, loss: 2.302130\n",
      "Epoch 104, loss: 2.302881\n",
      "Epoch 105, loss: 2.301548\n",
      "Epoch 106, loss: 2.301160\n",
      "Epoch 107, loss: 2.301245\n",
      "Epoch 108, loss: 2.302884\n",
      "Epoch 109, loss: 2.301700\n",
      "Epoch 110, loss: 2.301696\n",
      "Epoch 111, loss: 2.301636\n",
      "Epoch 112, loss: 2.302007\n",
      "Epoch 113, loss: 2.301283\n",
      "Epoch 114, loss: 2.301822\n",
      "Epoch 115, loss: 2.302556\n",
      "Epoch 116, loss: 2.300463\n",
      "Epoch 117, loss: 2.302126\n",
      "Epoch 118, loss: 2.302035\n",
      "Epoch 119, loss: 2.300712\n",
      "Epoch 120, loss: 2.302626\n",
      "Epoch 121, loss: 2.302190\n",
      "Epoch 122, loss: 2.302178\n",
      "Epoch 123, loss: 2.301928\n",
      "Epoch 124, loss: 2.301804\n",
      "Epoch 125, loss: 2.300053\n",
      "Epoch 126, loss: 2.301594\n",
      "Epoch 127, loss: 2.301393\n",
      "Epoch 128, loss: 2.301946\n",
      "Epoch 129, loss: 2.302271\n",
      "Epoch 130, loss: 2.301888\n",
      "Epoch 131, loss: 2.301359\n",
      "Epoch 132, loss: 2.301520\n",
      "Epoch 133, loss: 2.301142\n",
      "Epoch 134, loss: 2.302517\n",
      "Epoch 135, loss: 2.301544\n",
      "Epoch 136, loss: 2.302002\n",
      "Epoch 137, loss: 2.300274\n",
      "Epoch 138, loss: 2.300936\n",
      "Epoch 139, loss: 2.301086\n",
      "Epoch 140, loss: 2.301574\n",
      "Epoch 141, loss: 2.302698\n",
      "Epoch 142, loss: 2.301175\n",
      "Epoch 143, loss: 2.300697\n",
      "Epoch 144, loss: 2.301473\n",
      "Epoch 145, loss: 2.301955\n",
      "Epoch 146, loss: 2.301286\n",
      "Epoch 147, loss: 2.302138\n",
      "Epoch 148, loss: 2.302064\n",
      "Epoch 149, loss: 2.301018\n",
      "Epoch 150, loss: 2.301753\n",
      "Epoch 151, loss: 2.302105\n",
      "Epoch 152, loss: 2.301315\n",
      "Epoch 153, loss: 2.301701\n",
      "Epoch 154, loss: 2.301252\n",
      "Epoch 155, loss: 2.301338\n",
      "Epoch 156, loss: 2.301863\n",
      "Epoch 157, loss: 2.301958\n",
      "Epoch 158, loss: 2.300715\n",
      "Epoch 159, loss: 2.301182\n",
      "Epoch 160, loss: 2.301429\n",
      "Epoch 161, loss: 2.302020\n",
      "Epoch 162, loss: 2.300861\n",
      "Epoch 163, loss: 2.300745\n",
      "Epoch 164, loss: 2.301622\n",
      "Epoch 165, loss: 2.301179\n",
      "Epoch 166, loss: 2.301165\n",
      "Epoch 167, loss: 2.302520\n",
      "Epoch 168, loss: 2.300646\n",
      "Epoch 169, loss: 2.300812\n",
      "Epoch 170, loss: 2.301066\n",
      "Epoch 171, loss: 2.300698\n",
      "Epoch 172, loss: 2.300832\n",
      "Epoch 173, loss: 2.301449\n",
      "Epoch 174, loss: 2.302067\n",
      "Epoch 175, loss: 2.300337\n",
      "Epoch 176, loss: 2.300300\n",
      "Epoch 177, loss: 2.301954\n",
      "Epoch 178, loss: 2.300901\n",
      "Epoch 179, loss: 2.302044\n",
      "Epoch 180, loss: 2.300342\n",
      "Epoch 181, loss: 2.300888\n",
      "Epoch 182, loss: 2.302150\n",
      "Epoch 183, loss: 2.300633\n",
      "Epoch 184, loss: 2.301901\n",
      "Epoch 185, loss: 2.299977\n",
      "Epoch 186, loss: 2.300947\n",
      "Epoch 187, loss: 2.301887\n",
      "Epoch 188, loss: 2.299279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189, loss: 2.300385\n",
      "Epoch 190, loss: 2.300787\n",
      "Epoch 191, loss: 2.300355\n",
      "Epoch 192, loss: 2.300661\n",
      "Epoch 193, loss: 2.300320\n",
      "Epoch 194, loss: 2.301855\n",
      "Epoch 195, loss: 2.300981\n",
      "Epoch 196, loss: 2.300832\n",
      "Epoch 197, loss: 2.300870\n",
      "Epoch 198, loss: 2.301332\n",
      "Epoch 199, loss: 2.300963\n",
      "==========\n",
      "learning rate = 1e-05 reg = 1e-05 accuracy = 0.109 \n",
      "==========\n",
      "Epoch 0, loss: 2.302315\n",
      "Epoch 1, loss: 2.302190\n",
      "Epoch 2, loss: 2.302998\n",
      "Epoch 3, loss: 2.302816\n",
      "Epoch 4, loss: 2.302115\n",
      "Epoch 5, loss: 2.302869\n",
      "Epoch 6, loss: 2.301306\n",
      "Epoch 7, loss: 2.302092\n",
      "Epoch 8, loss: 2.302830\n",
      "Epoch 9, loss: 2.302000\n",
      "Epoch 10, loss: 2.302212\n",
      "Epoch 11, loss: 2.302256\n",
      "Epoch 12, loss: 2.303143\n",
      "Epoch 13, loss: 2.302060\n",
      "Epoch 14, loss: 2.302119\n",
      "Epoch 15, loss: 2.303107\n",
      "Epoch 16, loss: 2.302305\n",
      "Epoch 17, loss: 2.302266\n",
      "Epoch 18, loss: 2.302091\n",
      "Epoch 19, loss: 2.301717\n",
      "Epoch 20, loss: 2.303312\n",
      "Epoch 21, loss: 2.302435\n",
      "Epoch 22, loss: 2.302707\n",
      "Epoch 23, loss: 2.302162\n",
      "Epoch 24, loss: 2.303090\n",
      "Epoch 25, loss: 2.302032\n",
      "Epoch 26, loss: 2.302480\n",
      "Epoch 27, loss: 2.301950\n",
      "Epoch 28, loss: 2.302019\n",
      "Epoch 29, loss: 2.302040\n",
      "Epoch 30, loss: 2.301030\n",
      "Epoch 31, loss: 2.302728\n",
      "Epoch 32, loss: 2.301681\n",
      "Epoch 33, loss: 2.302036\n",
      "Epoch 34, loss: 2.301520\n",
      "Epoch 35, loss: 2.301718\n",
      "Epoch 36, loss: 2.301304\n",
      "Epoch 37, loss: 2.302056\n",
      "Epoch 38, loss: 2.301557\n",
      "Epoch 39, loss: 2.302354\n",
      "Epoch 40, loss: 2.301796\n",
      "Epoch 41, loss: 2.302628\n",
      "Epoch 42, loss: 2.301723\n",
      "Epoch 43, loss: 2.302650\n",
      "Epoch 44, loss: 2.301687\n",
      "Epoch 45, loss: 2.301407\n",
      "Epoch 46, loss: 2.302556\n",
      "Epoch 47, loss: 2.302096\n",
      "Epoch 48, loss: 2.301860\n",
      "Epoch 49, loss: 2.300555\n",
      "Epoch 50, loss: 2.302221\n",
      "Epoch 51, loss: 2.303429\n",
      "Epoch 52, loss: 2.301576\n",
      "Epoch 53, loss: 2.302377\n",
      "Epoch 54, loss: 2.301847\n",
      "Epoch 55, loss: 2.303079\n",
      "Epoch 56, loss: 2.302749\n",
      "Epoch 57, loss: 2.301404\n",
      "Epoch 58, loss: 2.301619\n",
      "Epoch 59, loss: 2.301204\n",
      "Epoch 60, loss: 2.302199\n",
      "Epoch 61, loss: 2.300886\n",
      "Epoch 62, loss: 2.301099\n",
      "Epoch 63, loss: 2.301782\n",
      "Epoch 64, loss: 2.301995\n",
      "Epoch 65, loss: 2.301201\n",
      "Epoch 66, loss: 2.302576\n",
      "Epoch 67, loss: 2.301809\n",
      "Epoch 68, loss: 2.301517\n",
      "Epoch 69, loss: 2.302458\n",
      "Epoch 70, loss: 2.301294\n",
      "Epoch 71, loss: 2.301626\n",
      "Epoch 72, loss: 2.301398\n",
      "Epoch 73, loss: 2.300497\n",
      "Epoch 74, loss: 2.301693\n",
      "Epoch 75, loss: 2.299902\n",
      "Epoch 76, loss: 2.301482\n",
      "Epoch 77, loss: 2.301536\n",
      "Epoch 78, loss: 2.300400\n",
      "Epoch 79, loss: 2.301446\n",
      "Epoch 80, loss: 2.301023\n",
      "Epoch 81, loss: 2.300980\n",
      "Epoch 82, loss: 2.300450\n",
      "Epoch 83, loss: 2.301657\n",
      "Epoch 84, loss: 2.300856\n",
      "Epoch 85, loss: 2.300712\n",
      "Epoch 86, loss: 2.301302\n",
      "Epoch 87, loss: 2.301197\n",
      "Epoch 88, loss: 2.301936\n",
      "Epoch 89, loss: 2.301328\n",
      "Epoch 90, loss: 2.300916\n",
      "Epoch 91, loss: 2.301265\n",
      "Epoch 92, loss: 2.301855\n",
      "Epoch 93, loss: 2.300928\n",
      "Epoch 94, loss: 2.301136\n",
      "Epoch 95, loss: 2.300618\n",
      "Epoch 96, loss: 2.302359\n",
      "Epoch 97, loss: 2.301529\n",
      "Epoch 98, loss: 2.301557\n",
      "Epoch 99, loss: 2.300380\n",
      "Epoch 100, loss: 2.301720\n",
      "Epoch 101, loss: 2.300777\n",
      "Epoch 102, loss: 2.301361\n",
      "Epoch 103, loss: 2.302694\n",
      "Epoch 104, loss: 2.300441\n",
      "Epoch 105, loss: 2.301386\n",
      "Epoch 106, loss: 2.302496\n",
      "Epoch 107, loss: 2.300478\n",
      "Epoch 108, loss: 2.301408\n",
      "Epoch 109, loss: 2.299599\n",
      "Epoch 110, loss: 2.302257\n",
      "Epoch 111, loss: 2.301215\n",
      "Epoch 112, loss: 2.302572\n",
      "Epoch 113, loss: 2.301538\n",
      "Epoch 114, loss: 2.300648\n",
      "Epoch 115, loss: 2.302255\n",
      "Epoch 116, loss: 2.300639\n",
      "Epoch 117, loss: 2.301510\n",
      "Epoch 118, loss: 2.301625\n",
      "Epoch 119, loss: 2.300422\n",
      "Epoch 120, loss: 2.300031\n",
      "Epoch 121, loss: 2.299796\n",
      "Epoch 122, loss: 2.301150\n",
      "Epoch 123, loss: 2.301948\n",
      "Epoch 124, loss: 2.300574\n",
      "Epoch 125, loss: 2.300687\n",
      "Epoch 126, loss: 2.300652\n",
      "Epoch 127, loss: 2.301828\n",
      "Epoch 128, loss: 2.301843\n",
      "Epoch 129, loss: 2.302048\n",
      "Epoch 130, loss: 2.301265\n",
      "Epoch 131, loss: 2.300045\n",
      "Epoch 132, loss: 2.300772\n",
      "Epoch 133, loss: 2.301482\n",
      "Epoch 134, loss: 2.301633\n",
      "Epoch 135, loss: 2.299811\n",
      "Epoch 136, loss: 2.301455\n",
      "Epoch 137, loss: 2.300414\n",
      "Epoch 138, loss: 2.300452\n",
      "Epoch 139, loss: 2.300606\n",
      "Epoch 140, loss: 2.300590\n",
      "Epoch 141, loss: 2.301062\n",
      "Epoch 142, loss: 2.301628\n",
      "Epoch 143, loss: 2.301625\n",
      "Epoch 144, loss: 2.300109\n",
      "Epoch 145, loss: 2.301554\n",
      "Epoch 146, loss: 2.301314\n",
      "Epoch 147, loss: 2.300305\n",
      "Epoch 148, loss: 2.300987\n",
      "Epoch 149, loss: 2.300415\n",
      "Epoch 150, loss: 2.300913\n",
      "Epoch 151, loss: 2.299689\n",
      "Epoch 152, loss: 2.301308\n",
      "Epoch 153, loss: 2.301444\n",
      "Epoch 154, loss: 2.302192\n",
      "Epoch 155, loss: 2.300543\n",
      "Epoch 156, loss: 2.299878\n",
      "Epoch 157, loss: 2.300894\n",
      "Epoch 158, loss: 2.300033\n",
      "Epoch 159, loss: 2.300718\n",
      "Epoch 160, loss: 2.300659\n",
      "Epoch 161, loss: 2.299685\n",
      "Epoch 162, loss: 2.301111\n",
      "Epoch 163, loss: 2.302035\n",
      "Epoch 164, loss: 2.300865\n",
      "Epoch 165, loss: 2.299523\n",
      "Epoch 166, loss: 2.299915\n",
      "Epoch 167, loss: 2.300874\n",
      "Epoch 168, loss: 2.300832\n",
      "Epoch 169, loss: 2.301738\n",
      "Epoch 170, loss: 2.300205\n",
      "Epoch 171, loss: 2.300375\n",
      "Epoch 172, loss: 2.301302\n",
      "Epoch 173, loss: 2.302124\n",
      "Epoch 174, loss: 2.301331\n",
      "Epoch 175, loss: 2.300745\n",
      "Epoch 176, loss: 2.298939\n",
      "Epoch 177, loss: 2.300662\n",
      "Epoch 178, loss: 2.300131\n",
      "Epoch 179, loss: 2.301287\n",
      "Epoch 180, loss: 2.301413\n",
      "Epoch 181, loss: 2.300980\n",
      "Epoch 182, loss: 2.299583\n",
      "Epoch 183, loss: 2.301937\n",
      "Epoch 184, loss: 2.300801\n",
      "Epoch 185, loss: 2.299607\n",
      "Epoch 186, loss: 2.300142\n",
      "Epoch 187, loss: 2.300035\n",
      "Epoch 188, loss: 2.299417\n",
      "Epoch 189, loss: 2.300689\n",
      "Epoch 190, loss: 2.301337\n",
      "Epoch 191, loss: 2.300569\n",
      "Epoch 192, loss: 2.302324\n",
      "Epoch 193, loss: 2.298981\n",
      "Epoch 194, loss: 2.300749\n",
      "Epoch 195, loss: 2.301745\n",
      "Epoch 196, loss: 2.302434\n",
      "Epoch 197, loss: 2.299194\n",
      "Epoch 198, loss: 2.301041\n",
      "Epoch 199, loss: 2.302166\n",
      "==========\n",
      "learning rate = 1e-05 reg = 1e-06 accuracy = 0.112 \n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparametestrength\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "cntr = 1\n",
    "\n",
    "for rate in learning_rates:\n",
    "    \n",
    "    print(f\"\\n============Loop # {cntr}...============\")\n",
    "    \n",
    "    for strength in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        \n",
    "        classifier.fit(X = train_X, y = train_y,\n",
    "                              batch_size = batch_size,\n",
    "                              reg = strength,\n",
    "                              epochs = num_epochs,\n",
    "                              learning_rate = rate)\n",
    "        \n",
    "        pred = classifier.predict(val_X)\n",
    "        \n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        \n",
    "        print('==========\\nlearning rate =', rate, 'reg =', strength, 'accuracy =', accuracy, '\\n==========')\n",
    "        \n",
    "        if best_val_accuracy is None:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "            best_learning_rate = rate\n",
    "            best_reg_strength = strength\n",
    "            \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "            best_learning_rate = rate\n",
    "            best_reg_strength = strength\n",
    "            \n",
    "    cntr += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.228000\n",
      "best learning rates: 0.001000\n",
      "best reg strengths: 0.000100\n"
     ]
    }
   ],
   "source": [
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "print('best learning rates: %f' % best_learning_rate)\n",
    "print('best reg strengths: %f' % best_reg_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.195000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
