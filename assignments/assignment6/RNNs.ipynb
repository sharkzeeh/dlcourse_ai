{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnaGQyRmeqTp",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "outputId": "eb14be62-8df4-484d-898a-dd4bb47b1393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5MB 33kB/s \n",
            "\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0MB 213kB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 3.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "c3555ca7-bfeb-4f05-8830-b47a2fa46746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "08e19fe8-cc84-4145-c189-c66d9370e53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "c05418c0-3b44-4fcf-d370-05c64524f9e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "e0d79ad9-9ede-4d3e-bf66-8642a70248c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'DET', 'VERB', 'PRON', 'ADJ', 'NUM', 'PRT', 'CONJ', 'ADV', 'NOUN', '.', 'X', 'ADP'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "653d0723-6fe3-46c1-de78-c9a039329530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "af8734be-4454-4d7a-ae27-8c93cf809ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "a1c1ea4e-35c3-47f0-aace-3ee92bb621f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "4821143e-302e-4278-f74c-07198febfe66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "c7c55d3d-4315-44f3-dc76-bca103dfc59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((39, 4), (39, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
        "        self.lstm_model = nn.LSTM(input_size=word_emb_dim,\n",
        "                                  hidden_size=lstm_hidden_dim, \n",
        "                                  num_layers=lstm_layers_count,\n",
        "                                  )\n",
        "        \n",
        "        self.layer_out = nn.Linear(in_features=lstm_hidden_dim,\n",
        "                                   out_features=tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        res, _ = self.lstm_model(embeddings)\n",
        "        return self.layer_out(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "f175ef7e-5dd0-4e9f-98c4-0f77de8333ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "def compute_accuracy(logits, y_batch):\n",
        "    preds = torch.argmax(logits, dim=2)\n",
        "    y_batch_msk = (y_batch != torch.zeros_like(y_batch)).float()\n",
        "    n_correct_samples = torch.sum(y_batch_msk * (preds == y_batch).float())\n",
        "    return n_correct_samples, torch.sum(y_batch_msk)\n",
        "\n",
        "n_cor, n_total = compute_accuracy(logits, y_batch)\n",
        "n_cor.item() / n_total.item()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14130434782608695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "12c0e841-4ad6-4e87-c870-668c517f2bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "def compute_loss(logits, y_batch, criterion):\n",
        "    loss, i = 0, 0\n",
        "    for row in logits:\n",
        "        loss += criterion(row, y_batch[i])\n",
        "        i += 1\n",
        "    return loss\n",
        "\n",
        "compute_loss(logits, y_batch, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(80.2333, grad_fn=<ThAddBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = compute_loss(logits, y_batch, criterion)\n",
        "\n",
        "                epoch_loss += loss\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = compute_accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "2834979d-d0cb-4f9a-ad87-d78167a4d272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 18.85180, Accuracy = 72.20%: 100%|██████████| 572/572 [00:20<00:00, 27.69it/s]\n",
            "[1 / 50]   Val: Loss = 9.93530, Accuracy = 84.80%: 100%|██████████| 13/13 [00:00<00:00, 37.08it/s]\n",
            "[2 / 50] Train: Loss = 6.14005, Accuracy = 90.06%: 100%|██████████| 572/572 [00:19<00:00, 28.87it/s]\n",
            "[2 / 50]   Val: Loss = 7.27339, Accuracy = 89.26%: 100%|██████████| 13/13 [00:00<00:00, 36.35it/s]\n",
            "[3 / 50] Train: Loss = 4.11216, Accuracy = 93.30%: 100%|██████████| 572/572 [00:19<00:00, 29.41it/s]\n",
            "[3 / 50]   Val: Loss = 6.52541, Accuracy = 90.77%: 100%|██████████| 13/13 [00:00<00:00, 37.70it/s]\n",
            "[4 / 50] Train: Loss = 3.09109, Accuracy = 94.91%: 100%|██████████| 572/572 [00:19<00:00, 28.91it/s]\n",
            "[4 / 50]   Val: Loss = 6.20154, Accuracy = 91.79%: 100%|██████████| 13/13 [00:00<00:00, 36.57it/s]\n",
            "[5 / 50] Train: Loss = 2.44173, Accuracy = 95.94%: 100%|██████████| 572/572 [00:19<00:00, 29.05it/s]\n",
            "[5 / 50]   Val: Loss = 6.24351, Accuracy = 92.28%: 100%|██████████| 13/13 [00:00<00:00, 38.94it/s]\n",
            "[6 / 50] Train: Loss = 1.97918, Accuracy = 96.68%: 100%|██████████| 572/572 [00:20<00:00, 28.45it/s]\n",
            "[6 / 50]   Val: Loss = 6.39532, Accuracy = 92.47%: 100%|██████████| 13/13 [00:00<00:00, 36.43it/s]\n",
            "[7 / 50] Train: Loss = 1.62623, Accuracy = 97.27%: 100%|██████████| 572/572 [00:21<00:00, 26.64it/s]\n",
            "[7 / 50]   Val: Loss = 6.30996, Accuracy = 92.66%: 100%|██████████| 13/13 [00:00<00:00, 31.92it/s]\n",
            "[8 / 50] Train: Loss = 1.34139, Accuracy = 97.75%: 100%|██████████| 572/572 [00:21<00:00, 27.05it/s]\n",
            "[8 / 50]   Val: Loss = 6.48468, Accuracy = 92.66%: 100%|██████████| 13/13 [00:00<00:00, 34.13it/s]\n",
            "[9 / 50] Train: Loss = 1.11290, Accuracy = 98.14%: 100%|██████████| 572/572 [00:20<00:00, 28.08it/s]\n",
            "[9 / 50]   Val: Loss = 6.85378, Accuracy = 92.63%: 100%|██████████| 13/13 [00:00<00:00, 35.11it/s]\n",
            "[10 / 50] Train: Loss = 0.91729, Accuracy = 98.47%: 100%|██████████| 572/572 [00:20<00:00, 27.70it/s]\n",
            "[10 / 50]   Val: Loss = 6.85789, Accuracy = 92.72%: 100%|██████████| 13/13 [00:00<00:00, 36.48it/s]\n",
            "[11 / 50] Train: Loss = 0.75118, Accuracy = 98.77%: 100%|██████████| 572/572 [00:20<00:00, 27.39it/s]\n",
            "[11 / 50]   Val: Loss = 7.33126, Accuracy = 92.57%: 100%|██████████| 13/13 [00:00<00:00, 34.57it/s]\n",
            "[12 / 50] Train: Loss = 0.61560, Accuracy = 99.00%: 100%|██████████| 572/572 [00:20<00:00, 28.35it/s]\n",
            "[12 / 50]   Val: Loss = 7.57246, Accuracy = 92.58%: 100%|██████████| 13/13 [00:00<00:00, 36.23it/s]\n",
            "[13 / 50] Train: Loss = 0.49578, Accuracy = 99.22%: 100%|██████████| 572/572 [00:20<00:00, 28.47it/s]\n",
            "[13 / 50]   Val: Loss = 7.97819, Accuracy = 92.52%: 100%|██████████| 13/13 [00:00<00:00, 35.26it/s]\n",
            "[14 / 50] Train: Loss = 0.40283, Accuracy = 99.38%: 100%|██████████| 572/572 [00:20<00:00, 28.25it/s]\n",
            "[14 / 50]   Val: Loss = 8.27617, Accuracy = 92.45%: 100%|██████████| 13/13 [00:00<00:00, 33.69it/s]\n",
            "[15 / 50] Train: Loss = 0.32527, Accuracy = 99.51%: 100%|██████████| 572/572 [00:20<00:00, 28.37it/s]\n",
            "[15 / 50]   Val: Loss = 8.65449, Accuracy = 92.44%: 100%|██████████| 13/13 [00:00<00:00, 36.56it/s]\n",
            "[16 / 50] Train: Loss = 0.25854, Accuracy = 99.62%: 100%|██████████| 572/572 [00:19<00:00, 28.70it/s]\n",
            "[16 / 50]   Val: Loss = 8.79605, Accuracy = 92.55%: 100%|██████████| 13/13 [00:00<00:00, 36.47it/s]\n",
            "[17 / 50] Train: Loss = 0.21151, Accuracy = 99.70%: 100%|██████████| 572/572 [00:20<00:00, 27.73it/s]\n",
            "[17 / 50]   Val: Loss = 9.42098, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 35.66it/s]\n",
            "[18 / 50] Train: Loss = 0.17796, Accuracy = 99.75%: 100%|██████████| 572/572 [00:20<00:00, 28.36it/s]\n",
            "[18 / 50]   Val: Loss = 9.47741, Accuracy = 92.35%: 100%|██████████| 13/13 [00:00<00:00, 37.76it/s]\n",
            "[19 / 50] Train: Loss = 0.15852, Accuracy = 99.77%: 100%|██████████| 572/572 [00:20<00:00, 27.93it/s]\n",
            "[19 / 50]   Val: Loss = 9.97740, Accuracy = 92.31%: 100%|██████████| 13/13 [00:00<00:00, 35.87it/s]\n",
            "[20 / 50] Train: Loss = 0.13729, Accuracy = 99.80%: 100%|██████████| 572/572 [00:19<00:00, 29.05it/s]\n",
            "[20 / 50]   Val: Loss = 10.18156, Accuracy = 92.28%: 100%|██████████| 13/13 [00:00<00:00, 38.52it/s]\n",
            "[21 / 50] Train: Loss = 0.12612, Accuracy = 99.81%: 100%|██████████| 572/572 [00:19<00:00, 29.34it/s]\n",
            "[21 / 50]   Val: Loss = 10.58477, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 37.45it/s]\n",
            "[22 / 50] Train: Loss = 0.12056, Accuracy = 99.81%: 100%|██████████| 572/572 [00:19<00:00, 29.35it/s]\n",
            "[22 / 50]   Val: Loss = 10.64821, Accuracy = 92.34%: 100%|██████████| 13/13 [00:00<00:00, 37.86it/s]\n",
            "[23 / 50] Train: Loss = 0.11513, Accuracy = 99.82%: 100%|██████████| 572/572 [00:19<00:00, 29.09it/s]\n",
            "[23 / 50]   Val: Loss = 11.01487, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 37.98it/s]\n",
            "[24 / 50] Train: Loss = 0.11807, Accuracy = 99.81%: 100%|██████████| 572/572 [00:19<00:00, 29.07it/s]\n",
            "[24 / 50]   Val: Loss = 11.46059, Accuracy = 92.20%: 100%|██████████| 13/13 [00:00<00:00, 39.51it/s]\n",
            "[25 / 50] Train: Loss = 0.11068, Accuracy = 99.82%: 100%|██████████| 572/572 [00:19<00:00, 28.92it/s]\n",
            "[25 / 50]   Val: Loss = 11.66282, Accuracy = 92.20%: 100%|██████████| 13/13 [00:00<00:00, 35.42it/s]\n",
            "[26 / 50] Train: Loss = 0.10789, Accuracy = 99.81%: 100%|██████████| 572/572 [00:19<00:00, 28.72it/s]\n",
            "[26 / 50]   Val: Loss = 11.99477, Accuracy = 92.12%: 100%|██████████| 13/13 [00:00<00:00, 36.24it/s]\n",
            "[27 / 50] Train: Loss = 0.10382, Accuracy = 99.82%: 100%|██████████| 572/572 [00:19<00:00, 29.18it/s]\n",
            "[27 / 50]   Val: Loss = 12.32769, Accuracy = 92.15%: 100%|██████████| 13/13 [00:00<00:00, 36.51it/s]\n",
            "[28 / 50] Train: Loss = 0.09640, Accuracy = 99.83%: 100%|██████████| 572/572 [00:19<00:00, 28.80it/s]\n",
            "[28 / 50]   Val: Loss = 12.73716, Accuracy = 92.11%: 100%|██████████| 13/13 [00:00<00:00, 36.08it/s]\n",
            "[29 / 50] Train: Loss = 0.09451, Accuracy = 99.83%: 100%|██████████| 572/572 [00:19<00:00, 29.15it/s]\n",
            "[29 / 50]   Val: Loss = 12.67276, Accuracy = 92.05%: 100%|██████████| 13/13 [00:00<00:00, 37.85it/s]\n",
            "[30 / 50] Train: Loss = 0.10362, Accuracy = 99.82%: 100%|██████████| 572/572 [00:19<00:00, 28.77it/s]\n",
            "[30 / 50]   Val: Loss = 13.03131, Accuracy = 92.00%: 100%|██████████| 13/13 [00:00<00:00, 37.03it/s]\n",
            "[31 / 50] Train: Loss = 0.09424, Accuracy = 99.83%: 100%|██████████| 572/572 [00:19<00:00, 29.00it/s]\n",
            "[31 / 50]   Val: Loss = 13.17079, Accuracy = 92.08%: 100%|██████████| 13/13 [00:00<00:00, 36.61it/s]\n",
            "[32 / 50] Train: Loss = 0.08837, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.91it/s]\n",
            "[32 / 50]   Val: Loss = 13.46166, Accuracy = 92.00%: 100%|██████████| 13/13 [00:00<00:00, 36.33it/s]\n",
            "[33 / 50] Train: Loss = 0.08990, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.97it/s]\n",
            "[33 / 50]   Val: Loss = 13.93993, Accuracy = 91.84%: 100%|██████████| 13/13 [00:00<00:00, 37.06it/s]\n",
            "[34 / 50] Train: Loss = 0.08966, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.85it/s]\n",
            "[34 / 50]   Val: Loss = 14.22074, Accuracy = 91.87%: 100%|██████████| 13/13 [00:00<00:00, 38.96it/s]\n",
            "[35 / 50] Train: Loss = 0.11327, Accuracy = 99.79%: 100%|██████████| 572/572 [00:19<00:00, 29.36it/s]\n",
            "[35 / 50]   Val: Loss = 13.74063, Accuracy = 91.92%: 100%|██████████| 13/13 [00:00<00:00, 36.67it/s]\n",
            "[36 / 50] Train: Loss = 0.09920, Accuracy = 99.82%: 100%|██████████| 572/572 [00:20<00:00, 28.46it/s]\n",
            "[36 / 50]   Val: Loss = 14.28836, Accuracy = 91.90%: 100%|██████████| 13/13 [00:00<00:00, 37.70it/s]\n",
            "[37 / 50] Train: Loss = 0.08125, Accuracy = 99.85%: 100%|██████████| 572/572 [00:19<00:00, 29.54it/s]\n",
            "[37 / 50]   Val: Loss = 14.53133, Accuracy = 91.88%: 100%|██████████| 13/13 [00:00<00:00, 38.49it/s]\n",
            "[38 / 50] Train: Loss = 0.07865, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.76it/s]\n",
            "[38 / 50]   Val: Loss = 14.70369, Accuracy = 91.85%: 100%|██████████| 13/13 [00:00<00:00, 35.06it/s]\n",
            "[39 / 50] Train: Loss = 0.07898, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.97it/s]\n",
            "[39 / 50]   Val: Loss = 14.98812, Accuracy = 91.87%: 100%|██████████| 13/13 [00:00<00:00, 40.21it/s]\n",
            "[40 / 50] Train: Loss = 0.07948, Accuracy = 99.85%: 100%|██████████| 572/572 [00:19<00:00, 28.73it/s]\n",
            "[40 / 50]   Val: Loss = 15.09164, Accuracy = 91.86%: 100%|██████████| 13/13 [00:00<00:00, 36.50it/s]\n",
            "[41 / 50] Train: Loss = 0.12249, Accuracy = 99.77%: 100%|██████████| 572/572 [00:19<00:00, 28.61it/s]\n",
            "[41 / 50]   Val: Loss = 14.91109, Accuracy = 91.79%: 100%|██████████| 13/13 [00:00<00:00, 37.20it/s]\n",
            "[42 / 50] Train: Loss = 0.10041, Accuracy = 99.81%: 100%|██████████| 572/572 [00:19<00:00, 28.75it/s]\n",
            "[42 / 50]   Val: Loss = 15.19599, Accuracy = 91.84%: 100%|██████████| 13/13 [00:00<00:00, 40.21it/s]\n",
            "[43 / 50] Train: Loss = 0.07995, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.33it/s]\n",
            "[43 / 50]   Val: Loss = 15.48758, Accuracy = 91.80%: 100%|██████████| 13/13 [00:00<00:00, 37.21it/s]\n",
            "[44 / 50] Train: Loss = 0.07441, Accuracy = 99.85%: 100%|██████████| 572/572 [00:19<00:00, 28.84it/s]\n",
            "[44 / 50]   Val: Loss = 15.74899, Accuracy = 91.78%: 100%|██████████| 13/13 [00:00<00:00, 37.35it/s]\n",
            "[45 / 50] Train: Loss = 0.07488, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.47it/s]\n",
            "[45 / 50]   Val: Loss = 15.91284, Accuracy = 91.71%: 100%|██████████| 13/13 [00:00<00:00, 36.61it/s]\n",
            "[46 / 50] Train: Loss = 0.07557, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.81it/s]\n",
            "[46 / 50]   Val: Loss = 16.05416, Accuracy = 91.76%: 100%|██████████| 13/13 [00:00<00:00, 38.66it/s]\n",
            "[47 / 50] Train: Loss = 0.08443, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.48it/s]\n",
            "[47 / 50]   Val: Loss = 16.22934, Accuracy = 91.70%: 100%|██████████| 13/13 [00:00<00:00, 34.11it/s]\n",
            "[48 / 50] Train: Loss = 0.12413, Accuracy = 99.77%: 100%|██████████| 572/572 [00:20<00:00, 27.82it/s]\n",
            "[48 / 50]   Val: Loss = 16.23491, Accuracy = 91.65%: 100%|██████████| 13/13 [00:00<00:00, 37.53it/s]\n",
            "[49 / 50] Train: Loss = 0.08659, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.65it/s]\n",
            "[49 / 50]   Val: Loss = 16.39038, Accuracy = 91.72%: 100%|██████████| 13/13 [00:00<00:00, 37.65it/s]\n",
            "[50 / 50] Train: Loss = 0.07500, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.59it/s]\n",
            "[50 / 50]   Val: Loss = 16.56145, Accuracy = 91.71%: 100%|██████████| 13/13 [00:00<00:00, 37.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPYGCOMQA03j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy_on_test(model, data, batch_size=64):\n",
        "\n",
        "    val_accuracy, n_correct_samples, n_total = 0, 0, 0\n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        model.eval()\n",
        "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "        logits = model(X_batch)\n",
        "\n",
        "        cur_corr, cur_total = compute_accuracy(logits, y_batch)\n",
        "        n_correct_samples += cur_corr\n",
        "        n_total += cur_total\n",
        "\n",
        "    accuracy = float(n_correct_samples.item())/n_total.item()\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wT-zNTwOhQl",
        "colab_type": "code",
        "outputId": "f7adfa1f-bc53-4ec0-ee37-664f3a11d3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "acc = compute_accuracy_on_test(model, (X_test, y_test))\n",
        "print(f'base accuracy on the inference: {acc:.5f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base accuracy on the inference: 0.91772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08MVnhNXeqVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
        "        self.lstm_model = nn.LSTM(input_size=word_emb_dim,\n",
        "                                  hidden_size=lstm_hidden_dim, \n",
        "                                  num_layers=lstm_layers_count,\n",
        "                                  bidirectional=True)\n",
        "        \n",
        "        self.layer_out = nn.Linear(in_features=lstm_hidden_dim * 2,\n",
        "                                   out_features=tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        res, _ = self.lstm_model(embeddings)\n",
        "        return self.layer_out(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RkMyA9Z6V_T",
        "colab_type": "code",
        "outputId": "298eadd4-d73d-411b-d2c4-afed730eaf9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    lstm_layers_count=2,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model,\n",
        "    criterion, optimizer,\n",
        "    train_data=(X_train, y_train),\n",
        "    epochs_count=50,\n",
        "    batch_size=32,\n",
        "    val_data=(X_val, y_val),\n",
        "    val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 26.52399, Accuracy = 83.57%: 100%|██████████| 1143/1143 [00:47<00:00, 24.14it/s]\n",
            "[1 / 50]   Val: Loss = 23.60305, Accuracy = 91.61%: 100%|██████████| 13/13 [00:00<00:00, 16.20it/s]\n",
            "[2 / 50] Train: Loss = 9.71050, Accuracy = 93.85%: 100%|██████████| 1143/1143 [00:47<00:00, 23.84it/s]\n",
            "[2 / 50]   Val: Loss = 17.03887, Accuracy = 94.23%: 100%|██████████| 13/13 [00:00<00:00, 16.57it/s]\n",
            "[3 / 50] Train: Loss = 5.34899, Accuracy = 96.24%: 100%|██████████| 1143/1143 [00:47<00:00, 24.14it/s]\n",
            "[3 / 50]   Val: Loss = 14.92448, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 17.10it/s]\n",
            "[4 / 50] Train: Loss = 3.17571, Accuracy = 97.52%: 100%|██████████| 1143/1143 [00:46<00:00, 24.33it/s]\n",
            "[4 / 50]   Val: Loss = 15.17349, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 17.35it/s]\n",
            "[5 / 50] Train: Loss = 2.01535, Accuracy = 98.31%: 100%|██████████| 1143/1143 [00:47<00:00, 24.20it/s]\n",
            "[5 / 50]   Val: Loss = 14.77325, Accuracy = 96.00%: 100%|██████████| 13/13 [00:00<00:00, 17.76it/s]\n",
            "[6 / 50] Train: Loss = 1.31548, Accuracy = 98.84%: 100%|██████████| 1143/1143 [00:46<00:00, 24.32it/s]\n",
            "[6 / 50]   Val: Loss = 17.36995, Accuracy = 95.75%: 100%|██████████| 13/13 [00:00<00:00, 17.48it/s]\n",
            "[7 / 50] Train: Loss = 0.90963, Accuracy = 99.20%: 100%|██████████| 1143/1143 [00:46<00:00, 24.35it/s]\n",
            "[7 / 50]   Val: Loss = 12.79119, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 16.64it/s]\n",
            "[8 / 50] Train: Loss = 0.60424, Accuracy = 99.44%: 100%|██████████| 1143/1143 [00:47<00:00, 24.29it/s]\n",
            "[8 / 50]   Val: Loss = 17.04638, Accuracy = 96.09%: 100%|██████████| 13/13 [00:00<00:00, 16.29it/s]\n",
            "[9 / 50] Train: Loss = 0.54628, Accuracy = 99.52%: 100%|██████████| 1143/1143 [00:47<00:00, 23.94it/s]\n",
            "[9 / 50]   Val: Loss = 17.36178, Accuracy = 95.99%: 100%|██████████| 13/13 [00:00<00:00, 16.43it/s]\n",
            "[10 / 50] Train: Loss = 0.35667, Accuracy = 99.66%: 100%|██████████| 1143/1143 [00:47<00:00, 24.27it/s]\n",
            "[10 / 50]   Val: Loss = 16.98137, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 16.84it/s]\n",
            "[11 / 50] Train: Loss = 0.20245, Accuracy = 99.81%: 100%|██████████| 1143/1143 [00:46<00:00, 24.42it/s]\n",
            "[11 / 50]   Val: Loss = 21.97324, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 16.25it/s]\n",
            "[12 / 50] Train: Loss = 0.29002, Accuracy = 99.75%: 100%|██████████| 1143/1143 [00:47<00:00, 24.27it/s]\n",
            "[12 / 50]   Val: Loss = 19.17901, Accuracy = 96.08%: 100%|██████████| 13/13 [00:00<00:00, 17.57it/s]\n",
            "[13 / 50] Train: Loss = 0.31064, Accuracy = 99.74%: 100%|██████████| 1143/1143 [00:46<00:00, 24.57it/s]\n",
            "[13 / 50]   Val: Loss = 23.22191, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 16.13it/s]\n",
            "[14 / 50] Train: Loss = 0.14736, Accuracy = 99.88%: 100%|██████████| 1143/1143 [00:46<00:00, 24.58it/s]\n",
            "[14 / 50]   Val: Loss = 18.89092, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 16.57it/s]\n",
            "[15 / 50] Train: Loss = 0.19581, Accuracy = 99.82%: 100%|██████████| 1143/1143 [00:46<00:00, 24.50it/s]\n",
            "[15 / 50]   Val: Loss = 17.18021, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 16.71it/s]\n",
            "[16 / 50] Train: Loss = 0.12782, Accuracy = 99.90%: 100%|██████████| 1143/1143 [00:46<00:00, 24.32it/s]\n",
            "[16 / 50]   Val: Loss = 21.49289, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 16.23it/s]\n",
            "[17 / 50] Train: Loss = 0.11160, Accuracy = 99.91%: 100%|██████████| 1143/1143 [00:46<00:00, 24.47it/s]\n",
            "[17 / 50]   Val: Loss = 20.31661, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 17.05it/s]\n",
            "[18 / 50] Train: Loss = 0.08348, Accuracy = 99.93%: 100%|██████████| 1143/1143 [00:47<00:00, 24.26it/s]\n",
            "[18 / 50]   Val: Loss = 21.95016, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 16.28it/s]\n",
            "[19 / 50] Train: Loss = 0.24262, Accuracy = 99.80%: 100%|██████████| 1143/1143 [00:46<00:00, 24.41it/s]\n",
            "[19 / 50]   Val: Loss = 22.61231, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 15.76it/s]\n",
            "[20 / 50] Train: Loss = 0.10520, Accuracy = 99.91%: 100%|██████████| 1143/1143 [00:46<00:00, 24.42it/s]\n",
            "[20 / 50]   Val: Loss = 20.69026, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 17.30it/s]\n",
            "[21 / 50] Train: Loss = 0.08358, Accuracy = 99.95%: 100%|██████████| 1143/1143 [00:46<00:00, 24.48it/s]\n",
            "[21 / 50]   Val: Loss = 19.74816, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 17.35it/s]\n",
            "[22 / 50] Train: Loss = 0.08395, Accuracy = 99.94%: 100%|██████████| 1143/1143 [00:46<00:00, 24.50it/s]\n",
            "[22 / 50]   Val: Loss = 23.95274, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 16.77it/s]\n",
            "[23 / 50] Train: Loss = 0.09972, Accuracy = 99.92%: 100%|██████████| 1143/1143 [00:46<00:00, 24.49it/s]\n",
            "[23 / 50]   Val: Loss = 18.48005, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 17.20it/s]\n",
            "[24 / 50] Train: Loss = 0.12741, Accuracy = 99.91%: 100%|██████████| 1143/1143 [00:47<00:00, 24.19it/s]\n",
            "[24 / 50]   Val: Loss = 24.65155, Accuracy = 96.61%: 100%|██████████| 13/13 [00:00<00:00, 16.21it/s]\n",
            "[25 / 50] Train: Loss = 0.09155, Accuracy = 99.93%: 100%|██████████| 1143/1143 [00:47<00:00, 24.21it/s]\n",
            "[25 / 50]   Val: Loss = 24.59993, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 16.19it/s]\n",
            "[26 / 50] Train: Loss = 0.06800, Accuracy = 99.95%: 100%|██████████| 1143/1143 [00:46<00:00, 24.51it/s]\n",
            "[26 / 50]   Val: Loss = 21.72202, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 17.21it/s]\n",
            "[27 / 50] Train: Loss = 0.04196, Accuracy = 99.97%: 100%|██████████| 1143/1143 [00:46<00:00, 24.40it/s]\n",
            "[27 / 50]   Val: Loss = 26.69468, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 17.03it/s]\n",
            "[28 / 50] Train: Loss = 0.06091, Accuracy = 99.96%: 100%|██████████| 1143/1143 [00:46<00:00, 24.47it/s]\n",
            "[28 / 50]   Val: Loss = 23.74672, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 17.24it/s]\n",
            "[29 / 50] Train: Loss = 0.15359, Accuracy = 99.88%: 100%|██████████| 1143/1143 [00:46<00:00, 24.40it/s]\n",
            "[29 / 50]   Val: Loss = 22.24394, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 16.27it/s]\n",
            "[30 / 50] Train: Loss = 0.07134, Accuracy = 99.95%: 100%|██████████| 1143/1143 [00:46<00:00, 24.51it/s]\n",
            "[30 / 50]   Val: Loss = 20.06054, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 16.62it/s]\n",
            "[31 / 50] Train: Loss = 0.05135, Accuracy = 99.97%: 100%|██████████| 1143/1143 [00:46<00:00, 24.49it/s]\n",
            "[31 / 50]   Val: Loss = 18.89009, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 16.86it/s]\n",
            "[32 / 50] Train: Loss = 0.11585, Accuracy = 99.93%: 100%|██████████| 1143/1143 [00:46<00:00, 24.47it/s]\n",
            "[32 / 50]   Val: Loss = 21.95310, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 16.92it/s]\n",
            "[33 / 50] Train: Loss = 0.08642, Accuracy = 99.94%: 100%|██████████| 1143/1143 [00:46<00:00, 24.49it/s]\n",
            "[33 / 50]   Val: Loss = 26.26308, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 17.92it/s]\n",
            "[34 / 50] Train: Loss = 0.16603, Accuracy = 99.92%: 100%|██████████| 1143/1143 [00:46<00:00, 24.65it/s]\n",
            "[34 / 50]   Val: Loss = 25.12621, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 16.51it/s]\n",
            "[35 / 50] Train: Loss = 0.07008, Accuracy = 99.96%: 100%|██████████| 1143/1143 [00:46<00:00, 24.62it/s]\n",
            "[35 / 50]   Val: Loss = 22.05887, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 16.17it/s]\n",
            "[36 / 50] Train: Loss = 0.02164, Accuracy = 99.99%: 100%|██████████| 1143/1143 [00:46<00:00, 24.62it/s]\n",
            "[36 / 50]   Val: Loss = 23.88033, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 16.36it/s]\n",
            "[37 / 50] Train: Loss = 0.00451, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:46<00:00, 24.46it/s]\n",
            "[37 / 50]   Val: Loss = 24.81963, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 16.46it/s]\n",
            "[38 / 50] Train: Loss = 0.00255, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:46<00:00, 24.58it/s]\n",
            "[38 / 50]   Val: Loss = 20.42843, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 17.30it/s]\n",
            "[39 / 50] Train: Loss = 0.03877, Accuracy = 99.97%: 100%|██████████| 1143/1143 [00:46<00:00, 24.45it/s]\n",
            "[39 / 50]   Val: Loss = 27.52109, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 16.79it/s]\n",
            "[40 / 50] Train: Loss = 0.25186, Accuracy = 99.84%: 100%|██████████| 1143/1143 [00:46<00:00, 24.54it/s]\n",
            "[40 / 50]   Val: Loss = 23.56143, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 16.41it/s]\n",
            "[41 / 50] Train: Loss = 0.12515, Accuracy = 99.93%: 100%|██████████| 1143/1143 [00:46<00:00, 24.35it/s]\n",
            "[41 / 50]   Val: Loss = 20.96101, Accuracy = 96.56%: 100%|██████████| 13/13 [00:00<00:00, 16.42it/s]\n",
            "[42 / 50] Train: Loss = 0.03007, Accuracy = 99.98%: 100%|██████████| 1143/1143 [00:46<00:00, 24.60it/s]\n",
            "[42 / 50]   Val: Loss = 20.71874, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 17.40it/s]\n",
            "[43 / 50] Train: Loss = 0.00519, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:46<00:00, 24.39it/s]\n",
            "[43 / 50]   Val: Loss = 18.36081, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 16.74it/s]\n",
            "[44 / 50] Train: Loss = 0.00195, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:46<00:00, 24.38it/s]\n",
            "[44 / 50]   Val: Loss = 19.79941, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 16.92it/s]\n",
            "[45 / 50] Train: Loss = 0.00204, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:47<00:00, 24.22it/s]\n",
            "[45 / 50]   Val: Loss = 20.49135, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 18.00it/s]\n",
            "[46 / 50] Train: Loss = 0.00309, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:46<00:00, 24.36it/s]\n",
            "[46 / 50]   Val: Loss = 21.87231, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 16.86it/s]\n",
            "[47 / 50] Train: Loss = 0.21531, Accuracy = 99.83%: 100%|██████████| 1143/1143 [00:47<00:00, 24.08it/s]\n",
            "[47 / 50]   Val: Loss = 19.52374, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 17.07it/s]\n",
            "[48 / 50] Train: Loss = 0.13964, Accuracy = 99.90%: 100%|██████████| 1143/1143 [00:47<00:00, 24.15it/s]\n",
            "[48 / 50]   Val: Loss = 20.67652, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 16.44it/s]\n",
            "[49 / 50] Train: Loss = 0.03247, Accuracy = 99.98%: 100%|██████████| 1143/1143 [00:47<00:00, 24.05it/s]\n",
            "[49 / 50]   Val: Loss = 21.95337, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 16.99it/s]\n",
            "[50 / 50] Train: Loss = 0.00903, Accuracy = 100.00%: 100%|██████████| 1143/1143 [00:47<00:00, 23.90it/s]\n",
            "[50 / 50]   Val: Loss = 21.51408, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 16.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R71PCs2OlHC",
        "colab_type": "code",
        "outputId": "21f46318-c9cf-4499-86b4-a6e44743b0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "acc = compute_accuracy_on_test(model, (X_test, y_test))\n",
        "print(f'bilateral LSTM accuracy on the inference: {acc:.5f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bilateral LSTM accuracy on the inference: 0.96906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "outputId": "15ee7e60-2bf5-4103-a99b-e4b836b2028b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "outputId": "8f8a33bb-2980-47bc-aa01-99df52a013fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n",
        "\n",
        "        input_size = embeddings.shape[1]\n",
        "        self.lstm_model = nn.LSTM(input_size=input_size,\n",
        "                                  hidden_size=lstm_hidden_dim,\n",
        "                                  num_layers=lstm_layers_count,\n",
        "                                  bidirectional=True)\n",
        "        self.layer_out = nn.Linear(in_features=lstm_hidden_dim * 2,\n",
        "                                   out_features=tagset_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        res, _ = self.lstm_model(embeddings)\n",
        "        return self.layer_out(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "outputId": "b8fa13c5-05b4-47a8-ba85-7ef9c7ced19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=4e-3, weight_decay=5e-4)\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 23.36112, Accuracy = 88.45%: 100%|██████████| 572/572 [00:17<00:00, 33.07it/s]\n",
            "[1 / 50]   Val: Loss = 14.21233, Accuracy = 93.52%: 100%|██████████| 51/51 [00:00<00:00, 67.62it/s]\n",
            "[2 / 50] Train: Loss = 9.28903, Accuracy = 95.17%: 100%|██████████| 572/572 [00:17<00:00, 32.60it/s]\n",
            "[2 / 50]   Val: Loss = 10.91826, Accuracy = 95.10%: 100%|██████████| 51/51 [00:00<00:00, 69.48it/s]\n",
            "[3 / 50] Train: Loss = 6.57024, Accuracy = 96.24%: 100%|██████████| 572/572 [00:17<00:00, 33.17it/s]\n",
            "[3 / 50]   Val: Loss = 9.47586, Accuracy = 95.73%: 100%|██████████| 51/51 [00:00<00:00, 67.04it/s]\n",
            "[4 / 50] Train: Loss = 5.39692, Accuracy = 96.68%: 100%|██████████| 572/572 [00:17<00:00, 33.08it/s]\n",
            "[4 / 50]   Val: Loss = 8.80291, Accuracy = 95.90%: 100%|██████████| 51/51 [00:00<00:00, 65.95it/s]\n",
            "[5 / 50] Train: Loss = 4.58257, Accuracy = 97.05%: 100%|██████████| 572/572 [00:17<00:00, 32.97it/s]\n",
            "[5 / 50]   Val: Loss = 8.70775, Accuracy = 96.15%: 100%|██████████| 51/51 [00:00<00:00, 66.86it/s]\n",
            "[6 / 50] Train: Loss = 4.02929, Accuracy = 97.24%: 100%|██████████| 572/572 [00:17<00:00, 33.14it/s]\n",
            "[6 / 50]   Val: Loss = 8.74148, Accuracy = 96.07%: 100%|██████████| 51/51 [00:00<00:00, 65.48it/s]\n",
            "[7 / 50] Train: Loss = 3.70175, Accuracy = 97.38%: 100%|██████████| 572/572 [00:17<00:00, 32.81it/s]\n",
            "[7 / 50]   Val: Loss = 8.18131, Accuracy = 96.33%: 100%|██████████| 51/51 [00:00<00:00, 68.26it/s]\n",
            "[8 / 50] Train: Loss = 3.41819, Accuracy = 97.51%: 100%|██████████| 572/572 [00:17<00:00, 33.03it/s]\n",
            "[8 / 50]   Val: Loss = 7.72545, Accuracy = 96.43%: 100%|██████████| 51/51 [00:00<00:00, 66.97it/s]\n",
            "[9 / 50] Train: Loss = 3.18832, Accuracy = 97.64%: 100%|██████████| 572/572 [00:17<00:00, 33.32it/s]\n",
            "[9 / 50]   Val: Loss = 8.10102, Accuracy = 96.48%: 100%|██████████| 51/51 [00:00<00:00, 67.40it/s]\n",
            "[10 / 50] Train: Loss = 2.97342, Accuracy = 97.73%: 100%|██████████| 572/572 [00:17<00:00, 33.32it/s]\n",
            "[10 / 50]   Val: Loss = 7.61362, Accuracy = 96.55%: 100%|██████████| 51/51 [00:00<00:00, 66.09it/s]\n",
            "[11 / 50] Train: Loss = 2.89017, Accuracy = 97.79%: 100%|██████████| 572/572 [00:17<00:00, 32.98it/s]\n",
            "[11 / 50]   Val: Loss = 7.87305, Accuracy = 96.44%: 100%|██████████| 51/51 [00:00<00:00, 66.77it/s]\n",
            "[12 / 50] Train: Loss = 2.86162, Accuracy = 97.79%: 100%|██████████| 572/572 [00:17<00:00, 33.01it/s]\n",
            "[12 / 50]   Val: Loss = 7.56249, Accuracy = 96.67%: 100%|██████████| 51/51 [00:00<00:00, 68.35it/s]\n",
            "[13 / 50] Train: Loss = 2.55552, Accuracy = 97.95%: 100%|██████████| 572/572 [00:17<00:00, 32.93it/s]\n",
            "[13 / 50]   Val: Loss = 8.31634, Accuracy = 96.35%: 100%|██████████| 51/51 [00:00<00:00, 66.20it/s]\n",
            "[14 / 50] Train: Loss = 2.57680, Accuracy = 97.93%: 100%|██████████| 572/572 [00:17<00:00, 33.19it/s]\n",
            "[14 / 50]   Val: Loss = 7.08526, Accuracy = 96.73%: 100%|██████████| 51/51 [00:00<00:00, 69.65it/s]\n",
            "[15 / 50] Train: Loss = 2.47902, Accuracy = 98.02%: 100%|██████████| 572/572 [00:17<00:00, 32.66it/s]\n",
            "[15 / 50]   Val: Loss = 7.40138, Accuracy = 96.65%: 100%|██████████| 51/51 [00:00<00:00, 65.91it/s]\n",
            "[16 / 50] Train: Loss = 2.43913, Accuracy = 98.02%: 100%|██████████| 572/572 [00:17<00:00, 33.31it/s]\n",
            "[16 / 50]   Val: Loss = 8.15335, Accuracy = 96.26%: 100%|██████████| 51/51 [00:00<00:00, 69.55it/s]\n",
            "[17 / 50] Train: Loss = 2.27736, Accuracy = 98.11%: 100%|██████████| 572/572 [00:17<00:00, 33.39it/s]\n",
            "[17 / 50]   Val: Loss = 7.77705, Accuracy = 96.70%: 100%|██████████| 51/51 [00:00<00:00, 67.16it/s]\n",
            "[18 / 50] Train: Loss = 2.47823, Accuracy = 98.02%: 100%|██████████| 572/572 [00:17<00:00, 33.35it/s]\n",
            "[18 / 50]   Val: Loss = 8.09377, Accuracy = 96.64%: 100%|██████████| 51/51 [00:00<00:00, 66.82it/s]\n",
            "[19 / 50] Train: Loss = 2.18263, Accuracy = 98.15%: 100%|██████████| 572/572 [00:17<00:00, 32.93it/s]\n",
            "[19 / 50]   Val: Loss = 7.41951, Accuracy = 96.74%: 100%|██████████| 51/51 [00:00<00:00, 66.44it/s]\n",
            "[20 / 50] Train: Loss = 2.07630, Accuracy = 98.23%: 100%|██████████| 572/572 [00:17<00:00, 33.35it/s]\n",
            "[20 / 50]   Val: Loss = 7.79022, Accuracy = 96.57%: 100%|██████████| 51/51 [00:00<00:00, 69.60it/s]\n",
            "[21 / 50] Train: Loss = 2.03274, Accuracy = 98.26%: 100%|██████████| 572/572 [00:17<00:00, 33.10it/s]\n",
            "[21 / 50]   Val: Loss = 8.12452, Accuracy = 96.69%: 100%|██████████| 51/51 [00:00<00:00, 71.21it/s]\n",
            "[22 / 50] Train: Loss = 2.35367, Accuracy = 98.08%: 100%|██████████| 572/572 [00:17<00:00, 33.40it/s]\n",
            "[22 / 50]   Val: Loss = 8.49125, Accuracy = 96.62%: 100%|██████████| 51/51 [00:00<00:00, 67.22it/s]\n",
            "[23 / 50] Train: Loss = 2.09550, Accuracy = 98.22%: 100%|██████████| 572/572 [00:17<00:00, 33.49it/s]\n",
            "[23 / 50]   Val: Loss = 7.72308, Accuracy = 96.65%: 100%|██████████| 51/51 [00:00<00:00, 70.29it/s]\n",
            "[24 / 50] Train: Loss = 1.84796, Accuracy = 98.36%: 100%|██████████| 572/572 [00:16<00:00, 33.72it/s]\n",
            "[24 / 50]   Val: Loss = 8.01868, Accuracy = 96.56%: 100%|██████████| 51/51 [00:00<00:00, 66.51it/s]\n",
            "[25 / 50] Train: Loss = 1.85054, Accuracy = 98.37%: 100%|██████████| 572/572 [00:16<00:00, 33.76it/s]\n",
            "[25 / 50]   Val: Loss = 8.48458, Accuracy = 96.69%: 100%|██████████| 51/51 [00:00<00:00, 68.40it/s]\n",
            "[26 / 50] Train: Loss = 2.20781, Accuracy = 98.16%: 100%|██████████| 572/572 [00:17<00:00, 33.06it/s]\n",
            "[26 / 50]   Val: Loss = 7.41981, Accuracy = 96.71%: 100%|██████████| 51/51 [00:00<00:00, 69.91it/s]\n",
            "[27 / 50] Train: Loss = 2.02740, Accuracy = 98.27%: 100%|██████████| 572/572 [00:17<00:00, 33.04it/s]\n",
            "[27 / 50]   Val: Loss = 7.95101, Accuracy = 96.67%: 100%|██████████| 51/51 [00:00<00:00, 67.17it/s]\n",
            "[28 / 50] Train: Loss = 1.89160, Accuracy = 98.34%: 100%|██████████| 572/572 [00:17<00:00, 33.19it/s]\n",
            "[28 / 50]   Val: Loss = 7.72566, Accuracy = 96.66%: 100%|██████████| 51/51 [00:00<00:00, 67.84it/s]\n",
            "[29 / 50] Train: Loss = 1.83843, Accuracy = 98.38%: 100%|██████████| 572/572 [00:17<00:00, 33.31it/s]\n",
            "[29 / 50]   Val: Loss = 7.60776, Accuracy = 96.73%: 100%|██████████| 51/51 [00:00<00:00, 65.72it/s]\n",
            "[30 / 50] Train: Loss = 1.89212, Accuracy = 98.35%: 100%|██████████| 572/572 [00:17<00:00, 33.32it/s]\n",
            "[30 / 50]   Val: Loss = 7.28960, Accuracy = 96.75%: 100%|██████████| 51/51 [00:00<00:00, 69.15it/s]\n",
            "[31 / 50] Train: Loss = 1.85743, Accuracy = 98.35%: 100%|██████████| 572/572 [00:17<00:00, 33.09it/s]\n",
            "[31 / 50]   Val: Loss = 7.40996, Accuracy = 96.82%: 100%|██████████| 51/51 [00:00<00:00, 68.65it/s]\n",
            "[32 / 50] Train: Loss = 1.76264, Accuracy = 98.44%: 100%|██████████| 572/572 [00:17<00:00, 33.13it/s]\n",
            "[32 / 50]   Val: Loss = 7.70363, Accuracy = 96.77%: 100%|██████████| 51/51 [00:00<00:00, 69.91it/s]\n",
            "[33 / 50] Train: Loss = 1.74316, Accuracy = 98.44%: 100%|██████████| 572/572 [00:17<00:00, 33.63it/s]\n",
            "[33 / 50]   Val: Loss = 7.47482, Accuracy = 96.74%: 100%|██████████| 51/51 [00:00<00:00, 70.15it/s]\n",
            "[34 / 50] Train: Loss = 2.12678, Accuracy = 98.22%: 100%|██████████| 572/572 [00:17<00:00, 33.53it/s]\n",
            "[34 / 50]   Val: Loss = 7.95054, Accuracy = 96.66%: 100%|██████████| 51/51 [00:00<00:00, 68.54it/s]\n",
            "[35 / 50] Train: Loss = 1.78062, Accuracy = 98.42%: 100%|██████████| 572/572 [00:17<00:00, 33.53it/s]\n",
            "[35 / 50]   Val: Loss = 7.47085, Accuracy = 96.70%: 100%|██████████| 51/51 [00:00<00:00, 65.95it/s]\n",
            "[36 / 50] Train: Loss = 1.66029, Accuracy = 98.50%: 100%|██████████| 572/572 [00:17<00:00, 33.30it/s]\n",
            "[36 / 50]   Val: Loss = 8.49871, Accuracy = 96.83%: 100%|██████████| 51/51 [00:00<00:00, 72.51it/s]\n",
            "[37 / 50] Train: Loss = 1.56782, Accuracy = 98.57%: 100%|██████████| 572/572 [00:16<00:00, 33.94it/s]\n",
            "[37 / 50]   Val: Loss = 8.50492, Accuracy = 96.53%: 100%|██████████| 51/51 [00:00<00:00, 67.83it/s]\n",
            "[38 / 50] Train: Loss = 1.86690, Accuracy = 98.38%: 100%|██████████| 572/572 [00:17<00:00, 33.62it/s]\n",
            "[38 / 50]   Val: Loss = 8.48025, Accuracy = 96.55%: 100%|██████████| 51/51 [00:00<00:00, 67.58it/s]\n",
            "[39 / 50] Train: Loss = 2.36306, Accuracy = 98.11%: 100%|██████████| 572/572 [00:17<00:00, 33.42it/s]\n",
            "[39 / 50]   Val: Loss = 8.39305, Accuracy = 96.51%: 100%|██████████| 51/51 [00:00<00:00, 71.08it/s]\n",
            "[40 / 50] Train: Loss = 1.93533, Accuracy = 98.34%: 100%|██████████| 572/572 [00:16<00:00, 33.82it/s]\n",
            "[40 / 50]   Val: Loss = 8.47043, Accuracy = 96.83%: 100%|██████████| 51/51 [00:00<00:00, 64.18it/s]\n",
            "[41 / 50] Train: Loss = 1.42688, Accuracy = 98.64%: 100%|██████████| 572/572 [00:17<00:00, 33.51it/s]\n",
            "[41 / 50]   Val: Loss = 7.70899, Accuracy = 96.85%: 100%|██████████| 51/51 [00:00<00:00, 71.34it/s]\n",
            "[42 / 50] Train: Loss = 1.60198, Accuracy = 98.57%: 100%|██████████| 572/572 [00:16<00:00, 33.75it/s]\n",
            "[42 / 50]   Val: Loss = 9.04439, Accuracy = 96.24%: 100%|██████████| 51/51 [00:00<00:00, 69.18it/s]\n",
            "[43 / 50] Train: Loss = 2.13428, Accuracy = 98.23%: 100%|██████████| 572/572 [00:16<00:00, 34.09it/s]\n",
            "[43 / 50]   Val: Loss = 7.90244, Accuracy = 96.92%: 100%|██████████| 51/51 [00:00<00:00, 66.91it/s]\n",
            "[44 / 50] Train: Loss = 1.66057, Accuracy = 98.50%: 100%|██████████| 572/572 [00:16<00:00, 34.19it/s]\n",
            "[44 / 50]   Val: Loss = 8.09008, Accuracy = 96.64%: 100%|██████████| 51/51 [00:00<00:00, 72.41it/s]\n",
            "[45 / 50] Train: Loss = 1.50191, Accuracy = 98.60%: 100%|██████████| 572/572 [00:16<00:00, 33.97it/s]\n",
            "[45 / 50]   Val: Loss = 7.34654, Accuracy = 96.85%: 100%|██████████| 51/51 [00:00<00:00, 68.27it/s]\n",
            "[46 / 50] Train: Loss = 1.95410, Accuracy = 98.37%: 100%|██████████| 572/572 [00:16<00:00, 34.35it/s]\n",
            "[46 / 50]   Val: Loss = 8.78381, Accuracy = 96.42%: 100%|██████████| 51/51 [00:00<00:00, 70.31it/s]\n",
            "[47 / 50] Train: Loss = 1.94486, Accuracy = 98.32%: 100%|██████████| 572/572 [00:16<00:00, 33.86it/s]\n",
            "[47 / 50]   Val: Loss = 7.89825, Accuracy = 96.68%: 100%|██████████| 51/51 [00:00<00:00, 72.42it/s]\n",
            "[48 / 50] Train: Loss = 1.42843, Accuracy = 98.65%: 100%|██████████| 572/572 [00:16<00:00, 34.02it/s]\n",
            "[48 / 50]   Val: Loss = 7.07162, Accuracy = 96.93%: 100%|██████████| 51/51 [00:00<00:00, 68.60it/s]\n",
            "[49 / 50] Train: Loss = 1.53596, Accuracy = 98.61%: 100%|██████████| 572/572 [00:16<00:00, 33.65it/s]\n",
            "[49 / 50]   Val: Loss = 8.07821, Accuracy = 96.82%: 100%|██████████| 51/51 [00:00<00:00, 70.41it/s]\n",
            "[50 / 50] Train: Loss = 1.81251, Accuracy = 98.42%: 100%|██████████| 572/572 [00:16<00:00, 34.16it/s]\n",
            "[50 / 50]   Val: Loss = 8.40880, Accuracy = 96.57%: 100%|██████████| 51/51 [00:00<00:00, 70.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "outputId": "0daa0047-88dd-4929-95e9-ca4e002cc1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "acc = compute_accuracy_on_test(model, (X_test, y_test))\n",
        "print(f'Bidirectional pretrained LSTM accuracy on the inference: {acc:.5f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bidirectional pretrained LSTM accuracy on the inference: 0.96643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMbPw47x4vE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}